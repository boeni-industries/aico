{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AICO \u2013 The AI+Companion Project","text":"<p>Early Development</p> <p>This project is in very early development. Documentation will be updated as features are implemented.</p>"},{"location":"#project-vision","title":"Project Vision","text":"<p>Building not just another virtual assistant, but a real companion: an AI that is present, curious, and grows with you. The goal is an AI sidekick that is emotionally aware, even a little quirky, with a genuine sense of agency and presence\u2014something closer to a friend or co-adventurer than a glorified notepad.</p>"},{"location":"#core-principles","title":"Core Principles","text":"<ul> <li>AICO is an open experiment to create an emotionally present, interactive, and learning AI companion</li> <li>Agency over reactivity - it should act with its own basic motivations and initiative, not just follow orders</li> <li>Personality and values - moods, likes/dislikes, things it stands for</li> <li>Companionship comes before tasks</li> </ul>"},{"location":"#planned-features","title":"\ud83d\udd2e Planned Features","text":"<p>Implementation Status</p> <p>These features are planned but not yet implemented. Documentation will be added as development progresses.</p> <ul> <li>Audio-Visual Representation: More than a voice or chatbot</li> <li>Embodyment: Hologram, robotic, animated avatar or anything in between</li> <li>Emotion &amp; Behavior Recognition: Camera and mic integration for emotional awareness</li> <li>Personality &amp; Character: Develops quirks and values over time</li> <li>Confidante Mode: Listens, remembers, provides emotional support</li> <li>Agency &amp; Intrinsic Motivation: Proactive interaction and suggestions</li> <li>Continual Learning: Adapts to user preferences and communication style</li> <li>Trust &amp; Privacy First: Local-first architecture with user control</li> </ul>"},{"location":"#development-status","title":"Development Status","text":"<p>This project is in the conceptual and early development phase. Check back regularly for updates as we build AICO together.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>All code and progress are public and messy\u2014join us if you want, fork it for yourself, or just watch. If you want to add features, challenge assumptions, or help build the ethics framework, you're welcome.</p> <p>Ready to contribute? Start with our Contributing Guide.</p> <p>Author: Michael B\u00f6ni (boeni.industries)</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#project-summary","title":"Project Summary","text":"<p>AICO is an open-source experiment to build an emotionally present, embodied, and proactive AI companion\u2014meant to act as more of a confidante and sidekick than a traditional assistant. Unlike typical productivity-oriented virtual assistants, AICO is designed to sense and adapt to the user's moods, initiate engagement, and form an evolving, personality-rich relationship.</p> <p>Core Principles: - Autonomous agency - AICO has its own goals, interests, and curiosities that drive self-directed behavior and learning - Strong user-centric privacy - Local-first with full user control - Modular, extensible architecture - Prioritizes companionship and long-term learning - Real-time emotional awareness - Multi-modal emotion recognition and adaptation</p>"},{"location":"architecture/#system-features","title":"System Features","text":"<p>AICO's features are organized into logical modules for development and deployment:</p>"},{"location":"architecture/#conversation-interaction","title":"\ud83d\udde3\ufe0f Conversation &amp; Interaction","text":"<ul> <li>Chat Interface: Real-time text-based conversation</li> <li>Voice Interaction: Speech-to-text and text-to-speech processing</li> <li>Context Management: Conversation thread management and context switching</li> <li>Autonomous Agency: Multi-faceted self-directed behavior including:</li> <li>Goal Generation: Self-formulated objectives and sub-goals</li> <li>Curiosity-Driven Learning: Intrinsic motivation to explore and learn</li> <li>Interest Development: Autonomous preference formation and pursuit</li> <li>Planning &amp; Reasoning: Multi-step strategic thinking and adaptation</li> <li>Meta-Cognition: Self-awareness of learning progress and capabilities</li> <li>Multi-turn Dialogue: Complex conversation flow management</li> <li>Interruption Handling: Natural conversation interruption and resumption</li> </ul>"},{"location":"architecture/#intelligence-memory","title":"\ud83e\udde0 Intelligence &amp; Memory","text":"<ul> <li>Personality Engine: Dynamic personality modeling and adaptation</li> <li>Episodic Memory: Personal experience and interaction history</li> <li>Semantic Memory: Knowledge base and learned concepts</li> <li>Vector Storage: Embedding-based similarity search and retrieval</li> <li>Memory Consolidation: Long-term memory formation and optimization</li> <li>Context Retrieval: Relevant memory recall based on current situation</li> </ul>"},{"location":"architecture/#emotion-awareness","title":"\ud83d\ude0a Emotion &amp; Awareness","text":"<ul> <li>Facial Recognition: Computer vision-based emotion detection</li> <li>Voice Analysis: Audio-based emotion and sentiment recognition</li> <li>Text Sentiment: Natural language emotion understanding</li> <li>Behavioral Patterns: User habit and preference learning</li> <li>Mood Tracking: Long-term emotional state monitoring</li> <li>Empathetic Responses: Emotion-appropriate reaction generation</li> </ul>"},{"location":"architecture/#embodiment-presence","title":"\ud83c\udfad Embodiment &amp; Presence","text":"<ul> <li>Avatar System: Visual representation and animation</li> <li>Gesture Recognition: Body language understanding</li> <li>Spatial Awareness: Environmental context understanding</li> <li>Physical Presence: Desktop, mobile, or projected embodiment</li> <li>AR/VR Integration: Immersive interaction capabilities</li> <li>Multi-device Sync: Consistent presence across devices</li> </ul>"},{"location":"architecture/#privacy-security","title":"\ud83d\udd12 Privacy &amp; Security","text":"<ul> <li>Local Processing: Edge-first computation and storage</li> <li>Data Encryption: End-to-end encryption for all personal data</li> <li>Consent Management: Granular privacy control and permissions</li> <li>Audit Logging: Transparent data usage tracking</li> <li>Homomorphic Encryption: Privacy-preserving cloud computations</li> <li>Zero-knowledge Authentication: Secure access without data exposure</li> </ul>"},{"location":"architecture/#extensibility-integration","title":"\ud83d\udd0c Extensibility &amp; Integration","text":"<ul> <li>Plugin System: Community-developed extensions and skills</li> <li>API Gateway: Unified interface for all system components</li> <li>External Integrations: Calendar, email, smart home connectivity</li> <li>Custom Skills: User-defined behaviors and responses</li> <li>Developer Tools: SDKs and documentation for extensions</li> <li>Marketplace: Plugin discovery and distribution platform</li> <li>Automated Updates: Self-updating system with user control</li> <li>Self-Restart Management: Graceful restarts with state preservation</li> </ul>"},{"location":"architecture/#system-architecture","title":"System Architecture","text":"<p>AICO follows a modular, message-driven architecture designed for local-first privacy, extensibility, and autonomous behavior. The system is organized into modules containing related components that communicate through a central message bus, enabling loose coupling and real-time, event-driven interactions.</p>"},{"location":"architecture/#core-design-principles","title":"Core Design Principles","text":""},{"location":"architecture/#local-first-by-default","title":"\ud83c\udfe0 Local-First by Default","text":"<p>All personal data and core inference runs locally (PC, workstation, or powerful edge device). Only opt-in features use cloud or edge APIs when stronger compute is required.</p>"},{"location":"architecture/#containerized-modules","title":"\ud83d\udce6 Containerized Modules","text":"<p>Each subsystem (personality engine, memory, emotion recognition) runs as a separate service or container (Docker, Podman). This enables: - Platform flexibility - Easy updates - Strict isolation for privacy - Independent scaling</p>"},{"location":"architecture/#unified-api-gateway","title":"\ud83c\udf10 Unified API Gateway","text":"<p>A local gRPC or HTTP API combines all modules. All frontends (avatar, chat, desktop widget, AR/VR) communicate through this unified interface.</p>"},{"location":"architecture/#plugin-system","title":"\ud83d\udd0c Plugin System","text":"<p>Expose a plugin interface for community extensions (skills, connectors, UI themes) in scripting languages (Python, Node.js).</p>"},{"location":"architecture/#privacy-controller","title":"\ud83d\udd12 Privacy Controller","text":"<p>Central user-governed dashboard for: - Toggling features - Controlling cloud data sharing - Data lifecycle management - Audit logging</p>"},{"location":"architecture/#autonomous-agency-architecture","title":"Autonomous Agency Architecture","text":"<p>AICO's autonomous agency is built on a multi-layered architecture that enables genuine self-directed behavior:</p>"},{"location":"architecture/#agency-layers","title":"Agency Layers","text":""},{"location":"architecture/#goal-generation-layer","title":"\ud83c\udfaf Goal Generation Layer","text":"<ul> <li>Autonomous Goal Formation: Dynamic creation of objectives based on curiosity and interests</li> <li>Hierarchical Planning: Multi-level goal decomposition and strategic planning</li> <li>Goal Prioritization: Self-directed importance assessment and resource allocation</li> </ul>"},{"location":"architecture/#curiosity-exploration-layer","title":"\ud83d\udd0d Curiosity &amp; Exploration Layer","text":"<ul> <li>Intrinsic Motivation Engine: RND/ICM algorithms for curiosity-driven exploration</li> <li>Novelty Detection: Identification of new experiences and learning opportunities</li> <li>Interest Tracking: Development and evolution of autonomous preferences</li> </ul>"},{"location":"architecture/#planning-reasoning-layer","title":"\ud83e\udde0 Planning &amp; Reasoning Layer","text":"<ul> <li>Strategic Planning: MCTS-based multi-step decision making</li> <li>Behavior Trees: Goal-oriented action selection and execution</li> <li>Context Integration: Environmental awareness and situational reasoning</li> </ul>"},{"location":"architecture/#meta-cognitive-layer","title":"\ud83e\ude9e Meta-Cognitive Layer","text":"<ul> <li>Self-Assessment: Understanding of own capabilities and limitations</li> <li>Learning Progress Monitoring: Awareness of knowledge acquisition and skill development</li> <li>Adaptive Behavior: Self-modification based on performance and outcomes</li> </ul>"},{"location":"architecture/#agency-integration","title":"Agency Integration","text":"<ul> <li>Unified Agency Controller: Coordinates all autonomous behaviors</li> <li>Goal-Memory Interface: Links autonomous objectives with episodic/semantic memory</li> <li>Personality-Agency Fusion: Ensures autonomous behavior aligns with personality traits</li> <li>Human-Agency Balance: Maintains appropriate boundaries and user control</li> </ul>"},{"location":"architecture/#system-architecture_1","title":"System Architecture","text":""},{"location":"architecture/#core-components","title":"Core Components","text":"Component Purpose Technology Chat Engine Real-time conversation management and threading WebSocket, FastAPI, conversation state Context Manager Conversation context and thread management Redis, conversation graphs LLM Interface Language model integration and prompt management Llama.cpp, Ollama, OpenAI adapters Personality Engine Dynamic personality modeling and adaptation Local ML models, behavior trees Emotion Recognition Multi-modal emotion detection (visual, audio, text) ONNX models, TensorFlow Lite Emotion Simulation Sophisticated emotion generation using AppraisalCloudPCT Component Process Model, appraisal theory, optional cloud enhancement Memory System Episodic and semantic memory with encryption SQLite, DuckDB, LiteFS Vector Store Embedding storage and similarity search ChromaDB, Qdrant, FAISS Autonomous Agent Multi-faceted autonomous behavior system Goal generation, curiosity engine, planning system Goal System Dynamic goal formation and hierarchical planning MCTS, behavior trees, goal-conditioned RL Curiosity Engine Intrinsic motivation and exploration drive RND, ICM, novelty detection algorithms Planning System Strategic reasoning and multi-step execution Monte Carlo Tree Search, hierarchical planning Voice &amp; Audio Speech-to-text and text-to-speech Whisper.cpp, Coqui, Piper Avatar System Real-time 3D avatar with lip-sync and expressions Three.js, Ready Player Me, TalkingHead.js Privacy Controller Advanced privacy and consent management Homomorphic encryption, ZK proofs API Gateway Unified interface for all modules FastAPI/gRPC, local web server Plugin Manager Dynamic plugin loading and management Hot-reload system, sandboxing Update Manager Automated system updates with rollback Version control, delta updates, integrity checks Restart Controller Graceful system restarts and recovery Process management, state persistence, health monitoring"},{"location":"architecture/#technology-stack","title":"Technology Stack","text":""},{"location":"architecture/#interface-layer","title":"Interface Layer","text":"<ul> <li>Primary UI: Flutter cross-platform application</li> <li>Avatar Integration: WebView embedding Three.js + Ready Player Me + TalkingHead.js</li> <li>Native Performance: Flutter for app logic, WebView for 3D avatar rendering</li> <li>JavaScript Bridge: Real-time communication between Flutter and avatar system</li> <li>Management Panel: Flutter-based settings and data management UI</li> <li>Advanced Embodiment: Unity/Unreal for future AR/VR expansion</li> </ul>"},{"location":"architecture/#aiml-layer","title":"AI/ML Layer","text":"<ul> <li>Local LLMs: Llama.cpp, Ollama, Mistral (quantized models)</li> <li>Autonomous Agent Frameworks: LangChain, LangGraph, CrewAI for agent orchestration</li> <li>Goal-Conditioned RL: HER (Hindsight Experience Replay), GCPO for goal-oriented learning</li> <li>Curiosity Algorithms: RND (Random Network Distillation), ICM (Intrinsic Curiosity Module)</li> <li>Planning Systems: MCTS (Monte Carlo Tree Search), hierarchical planning algorithms</li> <li>Behavior Trees: Goal-oriented behavior modeling and execution</li> <li>Emotion Models: ONNX Runtime, OpenVINO for edge inference</li> <li>Cloud Fallback: OpenAI, Gemini adapters (full opt-in)</li> <li>Voice Processing: Whisper.cpp (STT), Coqui/Piper (TTS)</li> </ul>"},{"location":"architecture/#data-storage","title":"Data &amp; Storage","text":"<ul> <li>Local Database: SQLite, DuckDB for fast, private storage</li> <li>Vector Database: ChromaDB, Qdrant for embedding storage and similarity search</li> <li>Encryption: Encryption-at-rest for all personal data</li> <li>Memory Store: Episodic and semantic memory with lifecycle management</li> <li>Embedding Models: Sentence transformers for vector generation</li> </ul>"},{"location":"architecture/#communication-orchestration","title":"Communication &amp; Orchestration","text":"<ul> <li>Message Bus: ZeroMQ or MQTT for inter-module communication</li> <li>API Gateway: FastAPI/gRPC for unified module access</li> <li>Plugin System: Hot-reloadable modules with well-documented APIs</li> </ul>"},{"location":"architecture/#deployment-distribution","title":"Deployment &amp; Distribution","text":"<ul> <li>Containerization: Docker/Podman with optimized multi-stage builds</li> <li>Size Optimization: Alpine base images, shared layers, minimal footprint</li> <li>Packaging: One-click installers for Win/Mac/Linux</li> <li>Automated Updates: Configurable self-updating system with:</li> <li>Delta updates for minimal bandwidth usage</li> <li>Cryptographic signature verification</li> <li>Atomic updates with rollback capabilities</li> <li>User-configurable update schedules and approval levels</li> <li>Self-Restart System: Graceful restart management with:</li> <li>State persistence across restarts</li> <li>Health monitoring and automatic recovery</li> <li>Zero-downtime updates for non-critical components</li> <li>Conversation continuity preservation</li> <li>CI/CD: Rapid development, easy upgrades, continuous security review</li> </ul>"},{"location":"architecture/#privacy-security_1","title":"Privacy &amp; Security","text":""},{"location":"architecture/#data-governance","title":"Data Governance","text":"<ul> <li>Local-first: All personal data stays on user's device by default</li> <li>Explicit Consent: Clear opt-in for any cloud features</li> <li>Audit Logging: Detailed, user-facing logs for transparency</li> <li>Data Lifecycle: User controls retention, deletion, and export</li> </ul>"},{"location":"architecture/#security-measures","title":"Security Measures","text":"<ul> <li>Encryption-at-rest: All local data encrypted</li> <li>Homomorphic Encryption: Privacy-preserving cloud computations</li> <li>Differential Privacy: Analytics while preserving individual privacy</li> <li>Zero-knowledge Proofs: Authentication without revealing data</li> <li>Secure Multi-party Computation: Collaborative learning without data sharing</li> <li>Module Isolation: Containerized components with limited permissions</li> <li>API Security: Authenticated local API access</li> <li>Regular Security Reviews: Continuous security assessment in CI/CD</li> </ul>"},{"location":"architecture/#extensibility","title":"Extensibility","text":""},{"location":"architecture/#plugin-architecture","title":"Plugin Architecture","text":"<ul> <li>Well-documented APIs: Clear interfaces for community development</li> <li>Hot-reloadable Modules: Update plugins without system restart</li> <li>Sandboxed Execution: Safe plugin execution environment</li> <li>Community Marketplace: Future plugin discovery and sharing</li> </ul>"},{"location":"architecture/#integration-points","title":"Integration Points","text":"<ul> <li>Calendar/Email: User-controlled data import</li> <li>Smart Home: Optional IoT device integration</li> <li>External APIs: Modular connectors for various services</li> <li>Custom Skills: User-defined behaviors and responses</li> </ul> <p>This architecture balances AICO's goals of privacy, embodiment, and extensibility while leveraging modern best practices in modular agents, edge AI, and user-centric design.</p>"},{"location":"architecture/context/","title":"System Context Diagram","text":""},{"location":"architecture/context/#c4-model-level-1-system-context","title":"C4 Model - Level 1: System Context","text":"<p>This diagram shows AICO in the context of its users and external systems, illustrating the high-level relationships and data flows.</p> <pre><code>flowchart TB\n    %% Define nodes with proper sizing\n    User[\"User&lt;br/&gt;Individual seeking emotionally&lt;br/&gt;present AI companion\"]\n\n    subgraph AICO_Core [\"AICO Core System\"]\n        AICO[\"AICO&lt;br/&gt;Emotionally present AI companion&lt;br/&gt;with autonomous agency\"]\n    end\n\n    subgraph Embodiment [\"Embodiment Layer\"]\n        Devices[\"Embodiment Devices&lt;br/&gt;Desktop, mobile, robot,&lt;br/&gt;hologram, AR/VR\"]\n    end\n\n    ExtServices[\"External Services&lt;br/&gt;Optional cloud AI, calendar,&lt;br/&gt;email, smart home\"]\n    Marketplace[\"Plugin Marketplace&lt;br/&gt;Community extensions&lt;br/&gt;and skills\"]\n    RPM[\"Ready Player Me&lt;br/&gt;Avatar creation&lt;br/&gt;service\"]\n\n    %% Define relationships\n    User ---|\"Interacts\"| AICO\n    AICO ---|\"Embodies\"| Devices\n    User ---|\"Uses\"| Devices\n\n    AICO -.-&gt;|\"Uses optionally\"| ExtServices\n    AICO -.-&gt;|\"Downloads from\"| Marketplace\n    AICO -.-&gt;|\"Fetches avatars\"| RPM\n\n    %% Clean modern styling\n    classDef primary fill:#2563eb,stroke:#1d4ed8,stroke-width:2px,color:#fff\n    classDef secondary fill:#64748b,stroke:#475569,stroke-width:2px,color:#fff\n    classDef external fill:#f1f5f9,stroke:#cbd5e1,stroke-width:2px,color:#334155\n    classDef boundary fill:#f8fafc,stroke:#e2e8f0,stroke-width:1px,color:#64748b\n\n    class User primary\n    class AICO primary\n    class Devices secondary\n    class ExtServices,Marketplace,RPM external\n    class AICO_Core,Embodiment boundary</code></pre>"},{"location":"architecture/context/#system-purpose","title":"System Purpose","text":"<p>AICO is an emotionally present, embodied AI companion designed to act as a confidante and sidekick rather than a traditional productivity assistant. The system prioritizes:</p> <ul> <li>Companionship over tasks - Building genuine relationships and emotional connections</li> <li>Privacy-first design - Local processing with user-controlled data sharing</li> <li>Autonomous agency - Self-directed goals, curiosity, and proactive engagement</li> <li>Multi-modal embodiment - Visual, audio, and spatial presence</li> </ul>"},{"location":"architecture/context/#key-actors","title":"Key Actors","text":""},{"location":"architecture/context/#primary-user","title":"Primary User","text":"<ul> <li>Individual seeking AI companionship - People who want an emotionally aware, supportive AI presence that grows and adapts over time</li> <li>Builders and tinkerers - Technical users who value privacy and want to customize their AI companion</li> <li>Privacy-conscious users - Individuals who want AI benefits without sacrificing personal data control</li> </ul>"},{"location":"architecture/context/#system-components","title":"System Components","text":""},{"location":"architecture/context/#aico-core-system","title":"AICO Core System","text":"<ul> <li>AICO - The central AI companion with autonomous agency, privacy-first design, and multi-modal interaction capabilities</li> </ul>"},{"location":"architecture/context/#embodiment-layer","title":"Embodiment Layer","text":"<p>AICO can be embodied across various devices and form factors:</p> <ul> <li>Embodiment Devices - Consolidated representation of all possible embodiment forms:</li> <li>Desktop/Laptop (screen-based avatar)</li> <li>Mobile devices (portable interface)</li> <li>Physical robots (embodied presence)</li> <li>Holographic displays (3D projection)</li> <li>AR/VR headsets (immersive experience)</li> </ul>"},{"location":"architecture/context/#external-systems","title":"External Systems","text":""},{"location":"architecture/context/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>Ready Player Me - Avatar creation and customization service for visual embodiment</li> <li>Plugin Marketplace - Community-driven ecosystem for extending AICO's capabilities</li> </ul>"},{"location":"architecture/context/#optional-external-services-user-opt-in","title":"Optional External Services (User Opt-in)","text":"<ul> <li>External Services - Consolidated optional integrations including:</li> <li>Cloud LLM Services (OpenAI, Gemini) for enhanced AI capabilities</li> <li>Calendar/Email integration for context awareness</li> <li>Smart Home devices for environmental awareness</li> <li>All require explicit user opt-in and permission</li> </ul>"},{"location":"architecture/context/#data-flows","title":"Data Flows","text":""},{"location":"architecture/context/#core-interactions-bidirectional","title":"Core Interactions (Bidirectional)","text":"<ul> <li>User \u2194 AICO - Multi-modal communication (voice, text, gestures, emotions) with empathetic responses</li> <li>AICO \u2194 Embodiment Devices - AICO manifests through various physical and virtual forms</li> <li>User \u2194 Embodiment Devices - Direct interaction with the chosen embodiment medium</li> </ul>"},{"location":"architecture/context/#external-system-integration","title":"External System Integration","text":"<ul> <li>Avatar customization - One-way flow from Ready Player Me for visual representation</li> <li>Plugin extensions - One-way flow from marketplace for system capabilities</li> <li>Optional services - Bidirectional API calls to external services (user opt-in only)</li> <li>Context data - Selective data flows from external services (explicit user permission)</li> </ul>"},{"location":"architecture/context/#privacy-security-context","title":"Privacy &amp; Security Context","text":"<p>AICO's architecture emphasizes local-first processing with explicit user consent for any external data sharing:</p> <ul> <li>Default local processing - All personal data and core AI processing happens on user's device</li> <li>Opt-in cloud features - External AI services only used when user explicitly chooses</li> <li>Granular permissions - User controls exactly what data can be accessed by which systems</li> <li>Encrypted communications - All external API calls use secure, encrypted channels</li> <li>Audit transparency - User can see exactly what data flows to external systems and when</li> </ul>"},{"location":"architecture/context/#system-boundaries","title":"System Boundaries","text":"<p>The AICO System boundary encompasses all local processing, data storage, and core AI capabilities. External systems are only accessed with explicit user permission and for specific, well-defined purposes that enhance the companion experience while maintaining privacy and user control.</p> <p>This context establishes AICO as a privacy-respecting, locally-intelligent system that can optionally leverage external services to provide enhanced capabilities when the user chooses to do so.</p>"},{"location":"architecture/emotion_sim/","title":"Emotion Simulation Architecture","text":""},{"location":"architecture/emotion_sim/#overview","title":"Overview","text":"<p>This document describes the technical architecture for AICO's Emotion Simulation module, focusing on its integration with the message bus system and data exchange formats. For conceptual information about the emotion model, see <code>/docs/concepts/emotion/emotion_sim.md</code>.</p>"},{"location":"architecture/emotion_sim/#bus-integration-architecture","title":"Bus Integration Architecture","text":""},{"location":"architecture/emotion_sim/#message-bus-topics","title":"Message Bus Topics","text":"<p>The Emotion Simulation module participates in the following message bus topics:</p>"},{"location":"architecture/emotion_sim/#input-topics-subscriptions","title":"Input Topics (Subscriptions)","text":"<pre><code>- user.emotion.detected      # From Emotion Recognition\n- conversation.message       # From Chat Engine\n- conversation.context       # From Context Manager\n- personality.state         # From Personality Engine\n- memory.relevant           # From Memory System\n- voice.analysis           # From Voice &amp; Audio\n</code></pre>"},{"location":"architecture/emotion_sim/#output-topics-publications","title":"Output Topics (Publications)","text":"<pre><code>- emotion.state.current     # Current emotional state\n- emotion.expression.voice  # Voice synthesis parameters\n- emotion.expression.avatar # Avatar animation parameters\n- emotion.expression.text   # Text generation context\n- emotion.memory.store      # Emotional experiences to store\n</code></pre>"},{"location":"architecture/emotion_sim/#message-schemas","title":"Message Schemas","text":"<p>Detailed message format specifications are documented in <code>emotion_sim_msg.md</code>. These include illustrative JSON structures for all input and output message types used by the Emotion Simulation module.</p> <p>Key Message Types: - Input: <code>user.emotion.detected</code>, <code>conversation.message</code>, <code>conversation.context</code>, <code>personality.state</code> - Output: <code>emotion.state.current</code>, <code>emotion.expression.voice</code>, <code>emotion.expression.avatar</code>, <code>emotion.expression.text</code></p>"},{"location":"architecture/emotion_sim/#processing-pipeline","title":"Processing Pipeline","text":""},{"location":"architecture/emotion_sim/#1-input-aggregation","title":"1. Input Aggregation","text":"<p>The Emotion Simulation module subscribes to multiple input topics and aggregates them into a unified context:</p> <pre><code>class EmotionSimulationModule:\n    def __init__(self, message_bus):\n        self.bus = message_bus\n        self.current_context = EmotionalContext()\n\n        # Subscribe to input topics\n        self.bus.subscribe(\"user.emotion.detected\", self.on_user_emotion)\n        self.bus.subscribe(\"conversation.message\", self.on_conversation_message)\n        self.bus.subscribe(\"conversation.context\", self.on_conversation_context)\n        self.bus.subscribe(\"personality.state\", self.on_personality_state)\n\n    def on_user_emotion(self, message):\n        self.current_context.user_emotion = message['emotion']\n        self.current_context.emotion_modalities = message['modalities']\n        self.trigger_emotion_processing()\n\n    def trigger_emotion_processing(self):\n        if self.current_context.is_complete():\n            emotional_state = self.process_emotional_response()\n            self.publish_emotional_outputs(emotional_state)\n</code></pre>"},{"location":"architecture/emotion_sim/#2-appraisal-processing","title":"2. Appraisal Processing","text":"<p>The core AppraisalCloudPCT algorithm processes the aggregated context:</p> <pre><code>def process_emotional_response(self) -&gt; EmotionalState:\n    # Stage 1: Relevance Assessment\n    relevance = self.assess_relevance(\n        user_emotion=self.current_context.user_emotion,\n        message_content=self.current_context.message,\n        conversation_context=self.current_context.conversation\n    )\n\n    # Stage 2: Goal Impact Analysis\n    goal_impact = self.analyze_goal_impact(\n        relevance=relevance,\n        relationship_phase=self.current_context.conversation['relationship_phase'],\n        user_emotional_state=self.current_context.user_emotion\n    )\n\n    # Stage 3: Coping Assessment\n    coping_strategy = self.determine_coping_strategy(\n        goal_impact=goal_impact,\n        personality_traits=self.current_context.personality,\n        crisis_indicators=self.current_context.conversation.get('crisis_indicators', False)\n    )\n\n    # Stage 4: Social Appropriateness Check\n    regulated_response = self.apply_social_regulation(\n        raw_emotional_response=coping_strategy,\n        relationship_context=self.current_context.conversation,\n        personality_constraints=self.current_context.personality\n    )\n\n    return self.generate_cpm_emotional_state(regulated_response)\n</code></pre>"},{"location":"architecture/emotion_sim/#3-output-generation","title":"3. Output Generation","text":"<p>Generated emotional states are published to multiple output topics:</p> <p><pre><code>def publish_emotional_outputs(self, emotional_state: EmotionalState):\n    # Publish current emotional state\n    self.bus.publish(\"emotion.state.current\", {\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"source\": \"emotion_simulation\",\n        \"emotional_state\": emotional_state.to_dict()\n    })\n\n    # Generate and publish voice parameters\n    voice_params = self.generate_voice_parameters(emotional_state)\n    self.bus.publish(\"emotion.expression.voice\", voice_params)\n\n    # Generate and publish avatar parameters\n    avatar_params = self.generate_avatar_parameters(emotional_state)\n    self.bus.publish(\"emotion.expression.avatar\", avatar_params)\n\n    # Generate and publish text context\n    text_context = self.generate_text_context(emotional_state)\n    self.bus.publish(\"emotion.expression.text\", text_context)\n\n    # Store emotional experience for learning\n    experience = self.create_emotional_experience(emotional_state)\n    self.bus.publish(\"emotion.memory.store\", experience)\n## Component Integration\n\n### Downstream Consumers\n\n#### Voice &amp; Audio System\n- **Subscribes to**: `emotion.expression.voice`\n- **Uses**: Prosody parameters, emotional coloring, articulation style\n- **Integration**: Direct parameter mapping to TTS engine settings\n\n#### Avatar System\n- **Subscribes to**: `emotion.expression.avatar`\n- **Uses**: Facial expressions, body language, gaze behavior\n- **Integration**: Real-time animation parameter updates via WebView JavaScript bridge\n\n#### Chat Engine\n- **Subscribes to**: `emotion.expression.text`\n- **Uses**: Emotional tone, response approach, content guidance\n- **Integration**: LLM prompt injection with emotional context\n\n#### Memory System\n- **Subscribes to**: `emotion.memory.store`\n- **Uses**: Emotional experiences for learning and pattern recognition\n- **Integration**: Encrypted storage of emotional interaction patterns\n\n### Upstream Providers\n\n#### Emotion Recognition\n- **Provides**: Real-time user emotional state detection\n- **Message Rate**: ~10Hz during active interaction\n- **Latency Requirement**: &lt;100ms for real-time responsiveness\n\n#### Context Manager\n- **Provides**: Conversation context and relationship state\n- **Message Rate**: Per conversation turn + periodic updates\n- **Latency Requirement**: &lt;50ms for context updates\n\n#### Personality Engine\n- **Provides**: Current personality state and interaction preferences\n- **Message Rate**: On personality changes + periodic state broadcasts\n- **Latency Requirement**: &lt;200ms for personality updates\n\n## Performance Requirements\n\n### Latency Targets\n- **End-to-end emotion processing**: &lt;200ms from input to output\n- **Voice parameter generation**: &lt;50ms for real-time speech synthesis\n- **Avatar parameter generation**: &lt;33ms for 30fps animation updates\n- **Text context generation**: &lt;100ms for conversation flow\n\n### Throughput Requirements\n- **Concurrent users**: Single-user system (local processing)\n- **Message processing rate**: 100+ messages/second during active interaction\n- **Memory usage**: &lt;512MB for emotion processing components\n\n### Reliability Requirements\n- **Availability**: 99.9% uptime during user sessions\n- **Graceful degradation**: Fallback to neutral emotional state on processing failures\n- **Recovery time**: &lt;1 second for component restart\n\n## Module Components\n\nThe Emotion Simulation module consists of four core components that work together to process emotional responses:\n\n### 1. Input Aggregation Component\n\n**Purpose**: Collects and synchronizes inputs from multiple message bus topics into a unified emotional context.\n\n**Responsibilities**:\n- **Message Subscription**: Subscribes to all input topics (`user.emotion.detected`, `conversation.message`, `conversation.context`, `personality.state`)\n- **Context Assembly**: Aggregates incoming messages into a complete emotional processing context\n- **Temporal Synchronization**: Ensures all inputs are temporally aligned for coherent processing\n- **Completeness Validation**: Determines when sufficient context is available to trigger emotion processing\n- **Timeout Management**: Handles missing or delayed inputs with appropriate fallback strategies\n\n**Key Features**:\n- **Buffering**: Short-term message buffering to handle timing variations\n- **Priority Handling**: Prioritizes critical inputs (e.g., crisis indicators) for immediate processing\n- **State Tracking**: Maintains current context state across multiple processing cycles\n\n**Output**: Unified `EmotionalContext` object containing all necessary input data\n\n### 2. Appraisal Processing Component\n\n**Purpose**: Implements the core AppraisalCloudPCT algorithm to evaluate situational significance and generate emotional appraisals.\n\n**Responsibilities**:\n- **Relevance Assessment**: Evaluates \"Does this situation matter to me?\" based on user emotional state and context\n- **Goal Impact Analysis**: Determines \"What does this mean for my companion goals?\" considering relationship phase and user needs\n- **Coping Evaluation**: Assesses \"Can I handle this appropriately?\" based on personality traits and situation complexity\n- **Normative Checking**: Validates \"Is my response socially appropriate?\" considering relationship boundaries and social context\n\n**Processing Stages**:\n1. **Stage 1 - Relevance**: Calculates relevance score (0.0-1.0) based on user emotional intensity and interaction context\n2. **Stage 2 - Implication**: Analyzes impact on companion relationship goals (supportive, neutral, challenging)\n3. **Stage 3 - Coping**: Determines appropriate response capability and approach style\n4. **Stage 4 - Normative**: Applies social appropriateness filters and relationship boundary checks\n\n**Key Features**:\n- **Configurable Sensitivity**: Adjustable appraisal sensitivity parameters\n- **Context Weighting**: Different weights for various contextual factors\n- **Crisis Detection**: Special handling for crisis situations requiring immediate response\n\n**Output**: `AppraisalResult` containing relevance scores, goal impacts, and response strategies\n\n### 3. Emotion Regulation Component\n\n**Purpose**: Applies social, ethical, and personality constraints to ensure appropriate emotional responses.\n\n**Responsibilities**:\n- **Social Appropriateness**: Ensures emotional responses are suitable for the current relationship phase and social context\n- **Crisis Protocol**: Applies specialized emotional regulation during user crisis situations\n- **Personality Alignment**: Modulates emotional intensity and expression style based on personality traits\n- **Boundary Maintenance**: Enforces companion relationship boundaries and ethical constraints\n- **Intensity Modulation**: Adjusts emotional expression intensity based on user state and context\n\n**Regulation Strategies**:\n- **Intensity Scaling**: Reduces or amplifies emotional expression based on appropriateness\n- **Style Adaptation**: Modifies expression style (e.g., more gentle, more confident) based on context\n- **Crisis Override**: Special protocols for handling user emotional crises\n- **Relationship Respect**: Maintains appropriate emotional distance based on relationship development\n\n**Key Features**:\n- **Configurable Constraints**: Adjustable regulation strength and personality influence\n- **Multi-layered Filtering**: Multiple regulation passes for different constraint types\n- **Context Sensitivity**: Different regulation strategies for different situational contexts\n\n**Output**: Regulated `EmotionalState` with appropriate constraints applied\n\n### 4. Output Synthesis Component\n\n**Purpose**: Transforms the regulated emotional state into coordinated expression parameters for different modalities.\n\n**Responsibilities**:\n- **Voice Parameter Generation**: Creates prosodic and emotional coloring parameters for speech synthesis\n- **Avatar Parameter Generation**: Generates facial expression, body language, and gaze behavior parameters\n- **Text Context Generation**: Produces emotional tone and content guidance for LLM text generation\n- **Memory Experience Creation**: Formats emotional experiences for storage and learning\n- **Multi-modal Coordination**: Ensures consistent emotional expression across all output channels\n\n**Synthesis Processes**:\n- **CPM Component Mapping**: Maps 5-component emotional state to specific expression parameters\n- **Modality Translation**: Converts abstract emotional components to concrete expression parameters\n- **Synchronization**: Ensures temporal alignment of expression parameters across modalities\n- **Intensity Calibration**: Adjusts expression intensity for each modality's characteristics\n\n**Output Channels**:\n- **Voice**: Prosody, emotional coloring, articulation parameters\n- **Avatar**: Facial expressions, body language, gaze behavior\n- **Text**: Emotional tone, response approach, content guidance\n- **Memory**: Structured emotional experience data\n\n**Key Features**:\n- **Modality-Specific Optimization**: Tailored parameter generation for each expression channel\n- **Real-time Performance**: Optimized for low-latency parameter generation\n- **Consistency Maintenance**: Ensures coherent emotional expression across all modalities\n\n## Data Flow Architecture\n</code></pre> \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Emotion         \u2502    \u2502 Conversation    \u2502    \u2502 Personality     \u2502 \u2502 Recognition     \u2502\u2500\u2500\u2500\u25b6\u2502 Context         \u2502\u2500\u2500\u2500\u25b6\u2502 Engine          \u2502 \u2502                 \u2502    \u2502 Manager         \u2502    \u2502                 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502                       \u2502                       \u2502          \u2502                       \u2502                       \u2502          \u25bc                       \u25bc                       \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                    Message Bus (ZeroMQ/MQTT)                    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502                                  \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                    Emotion Simulation Module                    \u2502 \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502  \u2502 Input       \u2502  \u2502 Appraisal   \u2502  \u2502 Emotion     \u2502  \u2502 Output  \u2502 \u2502 \u2502  \u2502 Aggregation \u2502\u2500\u25b6\u2502 Processing  \u2502\u2500\u25b6\u2502 Regulation  \u2502\u2500\u25b6\u2502 Synthesis\u2502 \u2502 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502                                  \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                    Message Bus (ZeroMQ/MQTT)                    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502                       \u2502                       \u2502          \u25bc                       \u25bc                       \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Voice &amp; Audio   \u2502    \u2502 Avatar System   \u2502    \u2502 Chat Engine     \u2502 \u2502 System          \u2502    \u2502                 \u2502    \u2502                 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 <pre><code>## Configuration\n\nExample module configuration:\n\n### Module Configuration\n```yaml\nemotion_simulation:\n  processing:\n    appraisal_sensitivity: 0.7\n    regulation_strength: 0.8\n    personality_influence: 0.6\n\n  performance:\n    max_processing_latency_ms: 200\n    batch_size: 1\n    thread_pool_size: 4\n\n  message_bus:\n    broker_url: \"tcp://localhost:5555\"\n    input_topics:\n      - \"user.emotion.detected\"\n      - \"conversation.message\"\n      - \"conversation.context\"\n      - \"personality.state\"\n    output_topics:\n      - \"emotion.state.current\"\n      - \"emotion.expression.voice\"\n      - \"emotion.expression.avatar\"\n      - \"emotion.expression.text\"\n\n  cloud_enhancement:\n    enabled: false\n    anonymization_level: \"high\"\n    learning_participation: false\n</code></pre></p>"},{"location":"architecture/emotion_sim/#error-handling","title":"Error Handling","text":""},{"location":"architecture/emotion_sim/#fault-tolerance","title":"Fault Tolerance","text":"<ul> <li>Input timeout: Default to neutral emotional state after 500ms without required inputs</li> <li>Processing failure: Fallback to last known stable emotional state</li> <li>Output delivery failure: Retry with exponential backoff, max 3 attempts</li> <li>Component crash: Automatic restart with state recovery from last checkpoint</li> </ul>"},{"location":"architecture/emotion_sim/#monitoring","title":"Monitoring","text":"<ul> <li>Health checks: Periodic processing pipeline validation</li> <li>Performance metrics: Latency, throughput, error rates</li> <li>Emotional coherence: Validation of emotional state transitions</li> <li>User experience impact: Correlation with user satisfaction metrics</li> </ul>"},{"location":"architecture/emotion_sim_msg/","title":"Emotion Simulation Message Formats","text":""},{"location":"architecture/emotion_sim_msg/#overview","title":"Overview","text":"<p>This document defines the message schemas used by the Emotion Simulation module for integration with AICO's message bus system. These are illustrative JSON structures that demonstrate the expected data formats and field types for system integration.</p> <p>Note: These message formats are examples to illustrate the data structure and field types. Actual implementations may vary based on specific requirements and system constraints.</p>"},{"location":"architecture/emotion_sim_msg/#input-message-formats","title":"Input Message Formats","text":""},{"location":"architecture/emotion_sim_msg/#useremotiondetected","title":"<code>user.emotion.detected</code>","text":"<p>Emotional state information detected from user inputs across multiple modalities.</p> <pre><code>{\n  \"timestamp\": \"2025-07-29T15:34:48Z\",\n  \"source\": \"emotion_recognition\",\n  \"emotion\": {\n    \"primary\": \"frustrated\",\n    \"confidence\": 0.85,\n    \"secondary\": [\"tired\", \"overwhelmed\"],\n    \"valence\": -0.6,\n    \"arousal\": 0.7,\n    \"dominance\": 0.3\n  },\n  \"modalities\": {\n    \"facial\": [\"furrowed_brow\", \"tight_lips\"],\n    \"voice\": [\"elevated_pitch\", \"faster_speech\"],\n    \"text\": [\"negative_sentiment\", \"complaint_indicators\"]\n  }\n}\n</code></pre> <p>Field Descriptions: - <code>emotion.primary</code>: Primary detected emotion (string) - <code>emotion.confidence</code>: Detection confidence level (0.0-1.0) - <code>emotion.secondary</code>: Additional detected emotions (array of strings) - <code>emotion.valence</code>: Pleasure/displeasure dimension (-1.0 to 1.0) - <code>emotion.arousal</code>: Activation/energy level (0.0-1.0) - <code>emotion.dominance</code>: Control/power dimension (0.0-1.0) - <code>modalities.*</code>: Indicators from different detection channels</p>"},{"location":"architecture/emotion_sim_msg/#conversationmessage","title":"<code>conversation.message</code>","text":"<p>Current conversation message with analysis metadata.</p> <pre><code>{\n  \"timestamp\": \"2025-07-29T15:34:48Z\",\n  \"source\": \"chat_engine\",\n  \"message\": {\n    \"text\": \"I'm having a really tough day at work\",\n    \"type\": \"user_input\",\n    \"thread_id\": \"conv_12345\",\n    \"turn_number\": 15\n  },\n  \"analysis\": {\n    \"intent\": \"emotional_sharing\",\n    \"urgency\": \"medium\",\n    \"requires_response\": true\n  }\n}\n</code></pre> <p>Field Descriptions: - <code>message.text</code>: Actual message content (string) - <code>message.type</code>: Message type (user_input, system_response, etc.) - <code>message.thread_id</code>: Conversation thread identifier - <code>message.turn_number</code>: Sequential turn number in conversation - <code>analysis.intent</code>: Detected user intent (string) - <code>analysis.urgency</code>: Message urgency level (low, medium, high) - <code>analysis.requires_response</code>: Whether response is expected (boolean)</p>"},{"location":"architecture/emotion_sim_msg/#conversationcontext","title":"<code>conversation.context</code>","text":"<p>Broader conversation context and relationship state information.</p> <pre><code>{\n  \"timestamp\": \"2025-07-29T15:34:48Z\",\n  \"source\": \"context_manager\",\n  \"context\": {\n    \"current_topic\": \"work_stress\",\n    \"conversation_phase\": \"problem_sharing\",\n    \"session_duration_minutes\": 15,\n    \"relationship_phase\": \"established_trust\",\n    \"time_context\": \"evening_after_work\",\n    \"crisis_indicators\": false\n  },\n  \"recent_history\": {\n    \"last_5_topics\": [\"weekend_plans\", \"work_project\", \"family_call\", \"work_stress\"],\n    \"emotional_trajectory\": [\"neutral\", \"positive\", \"neutral\", \"negative\"]\n  }\n}\n</code></pre> <p>Field Descriptions: - <code>context.current_topic</code>: Current conversation topic (string) - <code>context.conversation_phase</code>: Phase of current conversation - <code>context.session_duration_minutes</code>: Length of current session - <code>context.relationship_phase</code>: Current relationship development stage - <code>context.time_context</code>: Temporal/situational context - <code>context.crisis_indicators</code>: Whether crisis situation detected (boolean) - <code>recent_history.*</code>: Historical context for pattern recognition</p>"},{"location":"architecture/emotion_sim_msg/#personalitystate","title":"<code>personality.state</code>","text":"<p>Current personality configuration and mood state.</p> <pre><code>{\n  \"timestamp\": \"2025-07-29T15:34:48Z\",\n  \"source\": \"personality_engine\",\n  \"traits\": {\n    \"extraversion\": 0.6,\n    \"agreeableness\": 0.8,\n    \"conscientiousness\": 0.7,\n    \"neuroticism\": 0.3,\n    \"openness\": 0.9\n  },\n  \"interaction_style\": {\n    \"primary\": \"supportive_advisor\",\n    \"communication_preference\": \"warm_direct\",\n    \"emotional_expression_level\": 0.7\n  },\n  \"current_mood\": {\n    \"baseline_valence\": 0.2,\n    \"energy_level\": 0.6,\n    \"social_engagement\": 0.8\n  }\n}\n</code></pre> <p>Field Descriptions: - <code>traits.*</code>: Big Five personality trait values (0.0-1.0) - <code>interaction_style.primary</code>: Primary interaction approach - <code>interaction_style.communication_preference</code>: Preferred communication style - <code>interaction_style.emotional_expression_level</code>: Expression intensity (0.0-1.0) - <code>current_mood.*</code>: Current mood state parameters</p>"},{"location":"architecture/emotion_sim_msg/#memoryrelevant","title":"<code>memory.relevant</code>","text":"<p>Relevant memory retrieval results for emotional context.</p> <pre><code>{\n  \"timestamp\": \"2025-07-29T15:34:48Z\",\n  \"source\": \"memory_system\",\n  \"query_context\": {\n    \"user_emotional_state\": \"frustrated\",\n    \"conversation_topic\": \"work_stress\",\n    \"relationship_phase\": \"established_trust\"\n  },\n  \"relevant_memories\": [\n    {\n      \"memory_id\": \"mem_12345\",\n      \"similarity_score\": 0.89,\n      \"memory_type\": \"emotional_interaction\",\n      \"context\": \"user_work_stress_previous\",\n      \"successful_response\": \"gentle_encouragement_with_practical_advice\",\n      \"outcome\": \"positive_user_feedback\"\n    },\n    {\n      \"memory_id\": \"mem_67890\",\n      \"similarity_score\": 0.76,\n      \"memory_type\": \"relationship_pattern\",\n      \"context\": \"user_prefers_validation_before_advice\",\n      \"interaction_style\": \"listen_first_then_suggest\",\n      \"effectiveness\": \"high\"\n    }\n  ],\n  \"emotional_patterns\": {\n    \"user_stress_triggers\": [\"work_deadlines\", \"team_conflicts\"],\n    \"effective_support_styles\": [\"empathetic_listening\", \"practical_suggestions\"],\n    \"relationship_preferences\": [\"gentle_approach\", \"respect_boundaries\"]\n  }\n}\n</code></pre> <p>Field Descriptions: - <code>query_context.*</code>: Context used for memory retrieval - <code>relevant_memories[]</code>: Array of relevant past interactions - <code>relevant_memories[].similarity_score</code>: Relevance score (0.0-1.0) - <code>relevant_memories[].memory_type</code>: Type of memory (emotional_interaction, relationship_pattern, etc.) - <code>emotional_patterns.*</code>: Learned patterns about user emotional responses</p>"},{"location":"architecture/emotion_sim_msg/#voiceanalysis","title":"<code>voice.analysis</code>","text":"<p>Voice analysis results providing emotional and prosodic information.</p> <pre><code>{\n  \"timestamp\": \"2025-07-29T15:34:48Z\",\n  \"source\": \"voice_audio_system\",\n  \"prosodic_features\": {\n    \"pitch_mean\": 180.5,\n    \"pitch_variance\": 25.3,\n    \"speech_rate\": 4.2,\n    \"volume_level\": 0.7,\n    \"pause_frequency\": 0.3\n  },\n  \"emotional_indicators\": {\n    \"stress_level\": 0.8,\n    \"fatigue_indicators\": 0.6,\n    \"confidence_level\": 0.3,\n    \"emotional_stability\": 0.4\n  },\n  \"speech_quality\": {\n    \"clarity\": 0.9,\n    \"fluency\": 0.7,\n    \"hesitation_markers\": [\"um\", \"uh\", \"like\"],\n    \"speech_disruptions\": 2\n  },\n  \"contextual_analysis\": {\n    \"urgency_detected\": false,\n    \"question_intonation\": false,\n    \"emotional_intensity\": 0.7,\n    \"conversational_engagement\": 0.8\n  }\n}\n</code></pre> <p>Field Descriptions: - <code>prosodic_features.*</code>: Basic voice characteristics (pitch in Hz, rate in words/sec) - <code>emotional_indicators.*</code>: Emotional state indicators (0.0-1.0) - <code>speech_quality.*</code>: Speech production quality metrics - <code>contextual_analysis.*</code>: Higher-level speech context analysis</p>"},{"location":"architecture/emotion_sim_msg/#output-message-formats","title":"Output Message Formats","text":""},{"location":"architecture/emotion_sim_msg/#emotionstatecurrent","title":"<code>emotion.state.current</code>","text":"<p>Current emotional state generated by the emotion simulation system.</p> <pre><code>{\n  \"timestamp\": \"2025-07-29T15:34:48Z\",\n  \"source\": \"emotion_simulation\",\n  \"emotional_state\": {\n    \"cognitive\": {\n      \"appraisal_relevance\": 0.9,\n      \"goal_impact\": \"supportive_opportunity\",\n      \"control_assessment\": \"high_capability\",\n      \"social_appropriateness\": \"empathetic_response\"\n    },\n    \"physiological\": {\n      \"arousal_level\": 0.7,\n      \"energy_state\": \"focused_calm\"\n    },\n    \"motivational\": {\n      \"action_tendency\": \"provide_emotional_support\",\n      \"approach_style\": \"gentle_but_confident\"\n    },\n    \"motor\": {\n      \"expression_intensity\": 0.6,\n      \"gesture_style\": \"reassuring_open\",\n      \"posture_state\": \"attentive_forward_lean\"\n    },\n    \"subjective\": {\n      \"feeling_state\": \"concerned_but_caring\",\n      \"emotional_label\": \"empathetic_determination\"\n    }\n  },\n  \"regulation\": {\n    \"applied\": true,\n    \"adjustments\": [\"reduced_intensity_for_user_state\", \"increased_warmth\"]\n  }\n}\n</code></pre> <p>Field Descriptions: - <code>emotional_state.cognitive.*</code>: Cognitive appraisal components - <code>emotional_state.physiological.*</code>: Physiological arousal and energy - <code>emotional_state.motivational.*</code>: Action tendencies and approach style - <code>emotional_state.motor.*</code>: Physical expression parameters - <code>emotional_state.subjective.*</code>: Conscious feeling state - <code>regulation.applied</code>: Whether emotion regulation was applied (boolean) - <code>regulation.adjustments</code>: List of regulation adjustments made</p>"},{"location":"architecture/emotion_sim_msg/#emotionexpressionvoice","title":"<code>emotion.expression.voice</code>","text":"<p>Voice synthesis parameters derived from emotional state.</p> <pre><code>{\n  \"timestamp\": \"2025-07-29T15:34:48Z\",\n  \"source\": \"emotion_simulation\",\n  \"voice_parameters\": {\n    \"prosody\": {\n      \"pitch_base\": 0.4,\n      \"pitch_variation\": 0.3,\n      \"speech_rate\": 0.6,\n      \"volume_level\": 0.5\n    },\n    \"emotional_coloring\": {\n      \"warmth\": 0.8,\n      \"concern_level\": 0.6,\n      \"confidence\": 0.7,\n      \"urgency\": 0.2\n    },\n    \"articulation\": {\n      \"clarity\": 0.9,\n      \"breath_pattern\": \"calm_steady\",\n      \"pause_style\": \"thoughtful_supportive\"\n    }\n  }\n}\n</code></pre> <p>Field Descriptions: - <code>voice_parameters.prosody.*</code>: Basic prosodic parameters (0.0-1.0) - <code>voice_parameters.emotional_coloring.*</code>: Emotional tone parameters (0.0-1.0) - <code>voice_parameters.articulation.*</code>: Speech articulation characteristics</p>"},{"location":"architecture/emotion_sim_msg/#emotionexpressionavatar","title":"<code>emotion.expression.avatar</code>","text":"<p>Avatar animation parameters for visual emotional expression.</p> <pre><code>{\n  \"timestamp\": \"2025-07-29T15:34:48Z\",\n  \"source\": \"emotion_simulation\",\n  \"avatar_parameters\": {\n    \"facial_expression\": {\n      \"primary\": \"concerned_but_confident\",\n      \"eyebrow_position\": 0.3,\n      \"eye_openness\": 0.8,\n      \"mouth_shape\": \"gentle_serious\",\n      \"micro_expressions\": [\"slight_head_tilt\", \"soft_eye_contact\"]\n    },\n    \"body_language\": {\n      \"posture\": \"attentive_forward_lean\",\n      \"hand_position\": \"open_reassuring\",\n      \"gesture_style\": \"minimal_supportive\",\n      \"overall_tension\": 0.4\n    },\n    \"gaze_behavior\": {\n      \"eye_contact_level\": 0.8,\n      \"gaze_direction\": \"direct_caring\",\n      \"blink_pattern\": \"natural_attentive\"\n    }\n  }\n}\n</code></pre> <p>Field Descriptions: - <code>avatar_parameters.facial_expression.*</code>: Facial animation parameters - <code>avatar_parameters.body_language.*</code>: Body posture and gesture parameters - <code>avatar_parameters.gaze_behavior.*</code>: Eye movement and attention parameters</p>"},{"location":"architecture/emotion_sim_msg/#emotionexpressiontext","title":"<code>emotion.expression.text</code>","text":"<p>Text generation context and emotional guidance for LLM.</p> <pre><code>{\n  \"timestamp\": \"2025-07-29T15:34:48Z\",\n  \"source\": \"emotion_simulation\",\n  \"text_context\": {\n    \"emotional_tone\": \"supportive_understanding\",\n    \"response_approach\": \"validate_then_support\",\n    \"communication_style\": {\n      \"directness\": 0.6,\n      \"warmth\": 0.8,\n      \"formality\": 0.3,\n      \"energy\": 0.5\n    },\n    \"content_guidance\": {\n      \"primary_intent\": \"emotional_validation\",\n      \"secondary_intent\": \"practical_support_offer\",\n      \"avoid_patterns\": [\"dismissive_language\", \"overly_cheerful_tone\"],\n      \"emphasize_patterns\": [\"acknowledgment\", \"understanding\", \"availability\"]\n    }\n  }\n}\n</code></pre> <p>Field Descriptions: - <code>text_context.emotional_tone</code>: Overall emotional tone for response - <code>text_context.response_approach</code>: Strategic approach to response - <code>text_context.communication_style.*</code>: Communication style parameters (0.0-1.0) - <code>text_context.content_guidance.*</code>: Content generation guidance</p>"},{"location":"architecture/emotion_sim_msg/#emotionmemorystore","title":"<code>emotion.memory.store</code>","text":"<p>Emotional experience data for storage and learning.</p> <pre><code>{\n  \"timestamp\": \"2025-07-29T15:34:48Z\",\n  \"source\": \"emotion_simulation\",\n  \"experience\": {\n    \"situation\": {\n      \"user_emotional_state\": \"frustrated_about_work\",\n      \"conversation_context\": \"evening_stress_sharing\",\n      \"relationship_phase\": \"established_trust\"\n    },\n    \"aico_response\": {\n      \"emotional_state\": \"empathetic_determination\",\n      \"approach_taken\": \"validate_then_support\",\n      \"expression_coordination\": \"gentle_reassuring\"\n    },\n    \"outcome_tracking\": {\n      \"user_feedback\": null,\n      \"effectiveness_score\": null,\n      \"learning_value\": \"high\"\n    }\n  }\n}\n</code></pre> <p>Field Descriptions: - <code>experience.situation.*</code>: Situational context of the emotional interaction - <code>experience.aico_response.*</code>: AICO's emotional response and approach - <code>experience.outcome_tracking.*</code>: Tracking data for learning and improvement</p>"},{"location":"architecture/emotion_sim_msg/#message-bus-topics-summary","title":"Message Bus Topics Summary","text":""},{"location":"architecture/emotion_sim_msg/#input-topics-subscriptions","title":"Input Topics (Subscriptions)","text":"<ul> <li><code>user.emotion.detected</code> - User emotional state detection</li> <li><code>conversation.message</code> - Current conversation messages</li> <li><code>conversation.context</code> - Conversation and relationship context</li> <li><code>personality.state</code> - Personality configuration and mood</li> <li><code>memory.relevant</code> - Relevant memory retrieval results</li> <li><code>voice.analysis</code> - Voice analysis results</li> </ul>"},{"location":"architecture/emotion_sim_msg/#output-topics-publications","title":"Output Topics (Publications)","text":"<ul> <li><code>emotion.state.current</code> - Current AICO emotional state</li> <li><code>emotion.expression.voice</code> - Voice synthesis parameters</li> <li><code>emotion.expression.avatar</code> - Avatar animation parameters</li> <li><code>emotion.expression.text</code> - Text generation context</li> <li><code>emotion.memory.store</code> - Emotional experiences for storage</li> </ul>"},{"location":"architecture/emotion_sim_msg/#implementation-notes","title":"Implementation Notes","text":""},{"location":"architecture/emotion_sim_msg/#data-types","title":"Data Types","text":"<ul> <li>Timestamps: ISO 8601 format (UTC)</li> <li>Confidence/Probability Values: Float (0.0-1.0)</li> <li>Emotional Labels: String identifiers (standardized vocabulary)</li> <li>Arrays: JSON arrays for multiple values</li> <li>Nested Objects: Hierarchical data organization</li> </ul>"},{"location":"architecture/emotion_sim_msg/#message-validation","title":"Message Validation","text":"<ul> <li>All messages should include <code>timestamp</code> and <code>source</code> fields</li> <li>Numeric values should be validated for expected ranges</li> <li>String fields should use standardized vocabularies where applicable</li> <li>Optional fields may be omitted but should not be null</li> </ul>"},{"location":"architecture/emotion_sim_msg/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Message sizes should be kept minimal for low-latency processing</li> <li>Complex nested structures should be avoided in high-frequency messages</li> <li>Binary data should be avoided in favor of parameter references</li> </ul>"},{"location":"architecture/modules/","title":"Component Diagram","text":""},{"location":"architecture/modules/#c4-model-level-2-component-diagram","title":"C4 Model - Level 2: Component Diagram","text":"<p>This diagram shows the internal structure of the AICO system, breaking it down into modules and components and their interactions.</p> <pre><code>flowchart TB\n    %% External actors and systems\n    User[\"User&lt;br/&gt;AI companion seeker\"]\n    ExtServices[\"External Services&lt;br/&gt;Optional cloud AI, integrations\"]\n    Marketplace[\"Plugin Marketplace&lt;br/&gt;Community extensions\"]\n    RPM[\"Ready Player Me&lt;br/&gt;Avatar service\"]\n        %% Third-party plugins (outside AICO system)\n    ThirdPartyPlugins[\"Third-Party Plugins&lt;br/&gt;External integrations\"]\n\n    %% AICO System - Message Bus Architecture\n    subgraph AICO_System [\"AICO System - Message Bus Architecture\"]\n\n        %% Central Message Bus\n        MessageBus[\"\ud83d\ude8c Message Bus&lt;br/&gt;Event routing hub\"]\n\n        %% Presentation module\n        subgraph Presentation [\"Presentation Module\"]\n            FlutterApp[\"Flutter App&lt;br/&gt;UI &amp; settings\"]\n            WebView[\"WebView&lt;br/&gt;3D Avatar\"]\n        end\n\n        %% Core AI module\n        subgraph Core_AI [\"Core AI Module\"]\n            ChatEngine[\"Chat Engine&lt;br/&gt;Conversation\"]\n            LLMInterface[\"LLM Interface&lt;br/&gt;Language models\"]\n            PersonalityEngine[\"Personality Engine&lt;br/&gt;Behavior\"]\n            EmotionRecognition[\"Emotion Recognition&lt;br/&gt;Multi-modal\"]\n            EmotionSimulation[\"Emotion Simulation&lt;br/&gt;Expression synthesis\"]\n        end\n\n        %% Agency module\n        subgraph Agency [\"Autonomous Agency Module\"]\n            AutonomousAgent[\"Autonomous Agent&lt;br/&gt;Coordination\"]\n            GoalSystem[\"Goal System&lt;br/&gt;Planning\"]\n            CuriosityEngine[\"Curiosity Engine&lt;br/&gt;Motivation\"]\n        end\n\n        %% Data module\n        subgraph Data [\"Data &amp; Memory Module\"]\n            MemorySystem[\"Memory System&lt;br/&gt;Storage\"]\n            VectorStore[\"Vector Store&lt;br/&gt;Embeddings\"]\n            ContextManager[\"Context Manager&lt;br/&gt;Threads\"]\n        end\n\n        %% I/O module\n        subgraph IO [\"I/O Module\"]\n            VoiceAudio[\"Voice &amp; Audio&lt;br/&gt;STT/TTS\"]\n            AvatarSystem[\"Avatar System&lt;br/&gt;3D rendering\"]\n        end\n\n        %% Infrastructure module\n        subgraph Infrastructure [\"Infrastructure Module\"]\n            APIGateway[\"API Gateway&lt;br/&gt;External API\"]\n            PrivacyController[\"Privacy Controller&lt;br/&gt;Encryption\"]\n            PluginManager[\"Plugin Manager&lt;br/&gt;Extensions\"]\n            UpdateManager[\"Update Manager&lt;br/&gt;System updates\"]\n            RestartController[\"Restart Controller&lt;br/&gt;Recovery\"]\n        end\n\n        %% Internal plugin module\n        subgraph Internal_Plugins [\"Internal Plugin Module\"]\n            CommunityPlugins[\"Community Plugins&lt;br/&gt;Marketplace\"]\n            CustomSkills[\"Custom Skills&lt;br/&gt;User behaviors\"]\n            DeveloperTools[\"Developer Tools&lt;br/&gt;SDKs\"]\n        end\n    end\n\n    %% External user connections\n    User --&gt; FlutterApp\n    User --&gt; WebView\n\n    %% External service connections\n    APIGateway -.-&gt; ExtServices\n    PluginManager -.-&gt; Marketplace\n    AvatarSystem -.-&gt; RPM\n\n    %% Third-party plugin connections (via Plugin Manager only)\n    PluginManager --&gt; ThirdPartyPlugins\n\n    %% UI connections\n    FlutterApp --&gt; APIGateway\n    WebView --&gt; AvatarSystem\n\n    %% Message Bus connections (star pattern)\n    APIGateway --&gt; MessageBus\n    MessageBus --&gt; Core_AI\n    MessageBus --&gt; Agency\n    MessageBus --&gt; Data\n    MessageBus --&gt; IO\n    MessageBus --&gt; Infrastructure\n    MessageBus --&gt; Internal_Plugins\n\n    %% Plugin management\n    PluginManager --&gt; Internal_Plugins\n\n    %% Clean modern styling\n    classDef primary fill:#2563eb,stroke:#1d4ed8,stroke-width:2px,color:#fff\n    classDef external fill:#f1f5f9,stroke:#cbd5e1,stroke-width:2px,color:#334155\n    classDef boundary fill:#f8fafc,stroke:#e2e8f0,stroke-width:1px,color:#64748b\n    classDef bus fill:#dc2626,stroke:#b91c1c,stroke-width:3px,color:#fff\n    classDef ai fill:#7c3aed,stroke:#6d28d9,stroke-width:2px,color:#fff\n    classDef agency fill:#059669,stroke:#047857,stroke-width:2px,color:#fff\n    classDef memory fill:#ea580c,stroke:#c2410c,stroke-width:2px,color:#fff\n    classDef io fill:#0891b2,stroke:#0e7490,stroke-width:2px,color:#fff\n    classDef infra fill:#64748b,stroke:#475569,stroke-width:2px,color:#fff\n    classDef plugin fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff\n\n    class User primary\n    class ExtServices,Marketplace,RPM external\n    class ThirdPartyPlugins plugin\n    class MessageBus bus\n    class FlutterApp,WebView primary\n    class ChatEngine,LLMInterface,PersonalityEngine,EmotionRecognition,EmotionSimulation ai\n    class AutonomousAgent,GoalSystem,CuriosityEngine agency\n    class MemorySystem,VectorStore,ContextManager memory\n    class VoiceAudio,AvatarSystem io\n    class APIGateway,PrivacyController,PluginManager,UpdateManager,RestartController infra\n    class CommunityPlugins,CustomSkills,DeveloperTools plugin\n    class AICO_System,Presentation,Core_AI,Agency,Data,IO,Infrastructure,Internal_Plugins boundary</code></pre>"},{"location":"architecture/modules/#architectural-approach","title":"Architectural Approach","text":"<p>AICO uses a Message Bus Architecture with modular design principles:</p> <ul> <li>Central Message Bus - All components communicate through ZeroMQ/MQTT event routing</li> <li>Loose Coupling - Components are independent and communicate only via the message bus</li> <li>Scalability - New components can be added without modifying existing ones</li> <li>Resilience - Component failures are isolated and don't cascade</li> <li>Event-Driven - Autonomous behaviors emerge from event patterns and reactions</li> </ul>"},{"location":"architecture/modules/#system-overview","title":"System Overview","text":"<p>The system is organized into modules containing related components that connect to the central message bus, enabling decoupled, event-driven interactions.</p>"},{"location":"architecture/modules/#system-modules","title":"System Modules","text":""},{"location":"architecture/modules/#presentation-module","title":"Presentation Module","text":"<ul> <li>Flutter App - Cross-platform UI for settings, chat interface, and user interactions</li> <li>WebView - 3D avatar rendering using Three.js, Ready Player Me, and TalkingHead.js</li> </ul>"},{"location":"architecture/modules/#core-ai-module","title":"Core AI Module","text":"<ul> <li>Chat Engine - Conversation management and real-time messaging coordination</li> <li>LLM Interface - Language model integration (Llama.cpp, Ollama) with optional cloud fallback</li> <li>Personality Engine - Dynamic personality modeling and behavioral adaptation</li> <li>Emotion Recognition - Multi-modal emotion detection (visual, audio, text)</li> <li>Emotion Simulation - Sophisticated emotion generation using AppraisalCloudPCT with cognitive appraisal processes for believable companion interactions</li> </ul>"},{"location":"architecture/modules/#autonomous-agency-module","title":"Autonomous Agency Module","text":"<ul> <li>Autonomous Agent - Central coordinator for self-directed behavior and goal management</li> <li>Goal System - Hierarchical planning using MCTS and behavior trees</li> <li>Curiosity Engine - Intrinsic motivation using RND and ICM algorithms</li> </ul>"},{"location":"architecture/modules/#data-memory-module","title":"Data &amp; Memory Module","text":"<ul> <li>Memory System - Episodic and semantic memory (SQLite, DuckDB)</li> <li>Vector Store - Embedding storage and similarity search (ChromaDB)</li> <li>Context Manager - Conversation context and thread management</li> </ul>"},{"location":"architecture/modules/#io-module","title":"I/O Module","text":"<ul> <li>Voice &amp; Audio - STT/TTS processing (Whisper.cpp, Coqui)</li> <li>Avatar System - 3D rendering with lip-sync and facial expressions</li> </ul>"},{"location":"architecture/modules/#infrastructure-module","title":"Infrastructure Module","text":"<ul> <li>API Gateway - External interface (FastAPI, gRPC)</li> <li>Privacy Controller - Encryption and consent management</li> <li>Plugin Manager - Extension system with hot-reload and sandboxing</li> <li>Update Manager - Automated system updates with rollback capabilities</li> <li>Restart Controller - Graceful system restarts and recovery</li> </ul>"},{"location":"architecture/modules/#internal-plugin-module","title":"Internal Plugin Module","text":"<ul> <li>Community Plugins - Hot-loadable extensions from the community marketplace</li> <li>Custom Skills - User-defined behaviors and responses</li> <li>Developer Tools - SDKs and documentation for extension development</li> </ul>"},{"location":"architecture/modules/#external-plugin-integration","title":"External Plugin Integration","text":"<ul> <li>Third-Party Plugins - External components that connect to AICO's message bus for smart home, productivity, and custom integrations</li> </ul>"},{"location":"architecture/modules/#central-communication","title":"Central Communication","text":"<ul> <li>Message Bus - Event routing and pub/sub messaging (ZeroMQ/MQTT)</li> </ul>"},{"location":"architecture/modules/#plugin-integration-strategy","title":"Plugin Integration Strategy","text":""},{"location":"architecture/modules/#security-model","title":"Security Model","text":"<ul> <li>Plugin Manager as Gateway - All third-party plugins must connect through the Plugin Manager</li> <li>No Direct Message Bus Access - Third-party plugins cannot directly access the central message bus</li> <li>Sandboxing &amp; Permissions - Plugins run in isolated environments with explicit permission grants</li> <li>API-Based Integration - Plugins interact via well-defined APIs rather than internal system access</li> </ul>"},{"location":"architecture/modules/#plugin-types","title":"Plugin Types","text":"<ol> <li>Community Plugins - Internal hot-loadable extensions from the marketplace</li> <li>Custom Skills - User-defined behaviors and responses</li> <li>Third-Party Plugins - External integrations (smart home, productivity tools)</li> <li>Developer Tools - SDKs and utilities for plugin development</li> </ol>"},{"location":"architecture/modules/#benefits-of-this-architecture","title":"Benefits of This Architecture","text":"<ul> <li>Extensibility - Easy addition of new capabilities without core system changes</li> <li>Security - Controlled access prevents malicious or buggy plugins from compromising the system</li> <li>Performance - Plugin failures don't affect core system stability</li> <li>Developer Experience - Clear APIs and tools for building extensions</li> </ul>"},{"location":"architecture/modules/#emotional-simulation-integration","title":"Emotional Simulation Integration","text":"<p>The Emotion Simulation component provides AICO with believable emotional presence:</p>"},{"location":"architecture/modules/#core-capabilities","title":"Core Capabilities","text":"<ul> <li>Appraisal-Based Emotions - Generates emotions through 4-stage cognitive appraisal process (Relevance \u2192 Implication \u2192 Coping \u2192 Normative)</li> <li>Multi-Dimensional States - Rich emotional representation with PAD compatibility plus extended dimensions</li> <li>Cross-Modal Expression - Coordinates emotional expression across voice, avatar, and text with sophisticated mapping</li> <li>Relationship Intelligence - Social context and relationship dynamics influence emotional appropriateness</li> <li>Crisis Handling - Built-in emotion regulation for extreme situations with automatic recovery</li> <li>Ethical Constraints - Social appropriateness checks ensure companion-suitable responses</li> <li>Optional Cloud Learning - Collective emotional intelligence improvement through privacy-preserving cloud enhancement</li> </ul>"},{"location":"architecture/modules/#integration-points","title":"Integration Points","text":"<ul> <li>Personality Engine \u2192 influences emotional tendencies and patterns</li> <li>Avatar System \u2192 receives emotional state for facial expressions and gestures</li> <li>Voice &amp; Audio \u2192 receives emotional context for voice synthesis and tone</li> <li>Chat Engine \u2192 receives emotional context for response generation and word choice</li> <li>Memory System \u2192 stores emotional experiences and retrieves emotionally relevant memories</li> <li>Emotion Recognition \u2192 uses detected user emotions to inform appropriate responses</li> </ul>"},{"location":"architecture/modules/#technical-implementation","title":"Technical Implementation","text":"<ul> <li>Emotional State Vector - [valence, arousal, dominance] values in [0,1] range</li> <li>Expression Synthesis - Converts internal emotional state to external manifestations</li> <li>Contextual Modulation - Adjusts emotional responses based on conversation context</li> <li>Personality Consistency - Ensures emotional patterns align with established personality traits</li> </ul>"},{"location":"architecture/modules/#key-data-flows","title":"Key Data Flows","text":""},{"location":"architecture/modules/#user-interaction-flow","title":"User Interaction Flow","text":"<ol> <li>User interacts with Flutter Application</li> <li>Flutter Application communicates through API Gateway</li> <li>Chat Engine processes conversations via Message Bus</li> <li>LLM Interface generates responses influenced by Personality Engine</li> <li>Avatar System renders responses with synchronized lip-sync and expressions</li> </ol>"},{"location":"architecture/modules/#autonomous-behavior-flow","title":"Autonomous Behavior Flow","text":"<ol> <li>Curiosity Engine identifies interesting patterns or gaps</li> <li>Autonomous Agent formulates goals based on curiosity and personality</li> <li>Goal System creates hierarchical plans using behavior trees</li> <li>Planning System executes multi-step strategies</li> <li>Memory System stores experiences and outcomes for future learning</li> </ol>"},{"location":"architecture/modules/#memory-context-flow","title":"Memory &amp; Context Flow","text":"<ol> <li>Context Manager maintains conversation threads and context</li> <li>Memory System stores episodic experiences and semantic knowledge</li> <li>Vector Store enables similarity-based memory retrieval</li> <li>Privacy Controller ensures all data remains encrypted and private</li> </ol>"},{"location":"architecture/modules/#technology-integration","title":"Technology Integration","text":""},{"location":"architecture/modules/#local-first-architecture","title":"Local-First Architecture","text":"<ul> <li>All core AI processing happens locally using quantized models</li> <li>Personal data never leaves the device without explicit user consent</li> <li>Optional cloud services only used when user opts-in</li> </ul>"},{"location":"architecture/modules/#multi-modal-integration","title":"Multi-Modal Integration","text":"<ul> <li>Voice processing for natural speech interaction</li> <li>Visual emotion recognition for understanding facial expressions</li> <li>Text analysis for sentiment and intent understanding</li> <li>Avatar embodiment for visual presence and non-verbal communication</li> </ul>"},{"location":"architecture/modules/#extensibility-updates","title":"Extensibility &amp; Updates","text":"<ul> <li>Plugin system allows community-driven feature extensions</li> <li>Hot-reload capabilities enable updates without system restart</li> <li>Automated update system maintains security and feature currency</li> <li>Rollback mechanisms ensure system stability</li> </ul>"},{"location":"architecture/modules/#plugin-integration-strategy_1","title":"Plugin Integration Strategy","text":""},{"location":"architecture/modules/#how-plugins-extend-aico","title":"How Plugins Extend AICO","text":"<p>Internal plugins connect to the Message Bus via the Plugin Manager. External third-party plugins connect ONLY through the Plugin Manager for security. This design enables:</p> <ol> <li>Controlled Access - Plugin Manager mediates all external plugin access to the system</li> <li>Security Sandboxing - Third-party plugins cannot directly access the message bus</li> <li>Permission Management - Plugin Manager enforces access controls and permissions</li> <li>Hot-Loading - New plugins can be added without system restart</li> <li>Universal Extension - Approved plugins can extend any container via managed message bus access</li> </ol>"},{"location":"architecture/modules/#plugin-types_1","title":"Plugin Types","text":"<ul> <li>Behavioral Plugins - Extend personality and autonomous behaviors</li> <li>I/O Plugins - Add new input/output modalities (sensors, displays)</li> <li>Integration Plugins - Connect to external services (smart home, productivity tools)</li> <li>AI Model Plugins - Add specialized AI capabilities (image generation, music)</li> <li>Embodiment Plugins - Support new physical forms (robots, AR glasses)</li> </ul>"},{"location":"architecture/modules/#plugin-architecture-benefits","title":"Plugin Architecture Benefits","text":"<ul> <li>Security First - All external plugins go through Plugin Manager security controls</li> <li>No Core Modification - Plugins extend functionality without changing core containers</li> <li>Event-Driven - Plugins react to system events and publish their own (via Plugin Manager)</li> <li>Composable - Multiple plugins can work together through managed message bus access</li> <li>Maintainable - Plugin failures don't affect core system stability</li> <li>Controlled Access - Plugin Manager enforces permissions and sandboxing</li> </ul>"},{"location":"architecture/modules/#complete-feature-coverage","title":"Complete Feature Coverage","text":"<p>\u2705 All features from architecture documentation are represented:</p> <ul> <li>Conversation &amp; Interaction - Chat Engine, LLM Interface, Context Manager</li> <li>Intelligence &amp; Memory - Memory System, Vector Store, Personality Engine</li> <li>Emotion &amp; Awareness - Emotion Recognition, multi-modal detection</li> <li>Embodiment &amp; Presence - Avatar System, Voice &amp; Audio, WebView rendering</li> <li>Privacy &amp; Security - Privacy Controller, local-first processing</li> <li>Extensibility &amp; Integration - Plugin Manager, Community Plugins, Custom Skills</li> <li>Autonomous Agency - Autonomous Agent, Goal System, Curiosity Engine</li> <li>Infrastructure - API Gateway, Message Bus, Update Manager, Restart Controller</li> </ul> <p>This container architecture enables AICO to function as a truly autonomous, emotionally aware companion while maintaining privacy, extensibility, and robust performance across different embodiment devices.</p>"},{"location":"concepts/emotion/emotion_sim/","title":"Emotion Simulation","text":""},{"location":"concepts/emotion/emotion_sim/#overview","title":"Overview","text":"<p>The Emotion Simulation component generates sophisticated emotional states using AppraisalCloudPCT (Component Process Model with cloud enhancement), creating believable emotional responses that enhance AICO's companion experience. This system processes contextual inputs through cognitive appraisal mechanisms, generating multi-dimensional emotional states that coordinate expression across voice, avatar, and text modalities.</p>"},{"location":"concepts/emotion/emotion_sim/#rationale","title":"Rationale","text":""},{"location":"concepts/emotion/emotion_sim/#why-appraisalcloudpct","title":"Why AppraisalCloudPCT?","text":"<p>AICO requires sophisticated emotional intelligence that goes beyond simple reactive responses. AppraisalCloudPCT provides:</p> <ul> <li>Human-Like Emotion Generation: Emotions emerge through cognitive appraisal processes, mirroring how humans actually experience emotions</li> <li>Context-Aware Responses: Situational evaluation determines appropriate emotional reactions</li> <li>Relationship Intelligence: Social context and relationship dynamics influence emotional appropriateness</li> <li>Crisis Handling: Built-in emotion regulation for extreme situations</li> <li>Continuous Learning: Optional cloud enhancement improves emotional intelligence over time</li> <li>Ethical Constraints: Social appropriateness checks ensure companion-suitable responses</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#component-process-model-foundation","title":"Component Process Model Foundation","text":"<p>AppraisalCloudPCT is based on Klaus Scherer's Component Process Model (CPM), the leading emotion theory in contemporary psychology. CPM explains emotions as emerging from a 4-stage appraisal process:</p> <p>Stage 1: Relevance Check - \"Does this event matter to me?\" - Determines if emotional response is warranted - For AICO: Does this conversation event require emotional attention?</p> <p>Stage 2: Implication Check - \"What does this mean for my goals?\" - Evaluates goal conduciveness/obstruction - For AICO: Does this help or hinder my companion objectives?</p> <p>Stage 3: Coping Check - \"Can I handle this situation?\" - Assesses control and power dynamics - For AICO: What's the appropriate assertiveness level?</p> <p>Stage 4: Normative Check - \"Is this consistent with my values?\" - Evaluates moral/social appropriateness - For AICO: Does this align with my personality and relationship norms?</p>"},{"location":"concepts/emotion/emotion_sim/#architecture","title":"Architecture","text":""},{"location":"concepts/emotion/emotion_sim/#appraisalcloudpct-components","title":"AppraisalCloudPCT Components","text":"<p>AICO's emotion simulation consists of five integrated components:</p>"},{"location":"concepts/emotion/emotion_sim/#1-appraisal-engine","title":"1. Appraisal Engine","text":"<p>Processes conversation events through the 4-stage appraisal sequence:</p> <pre><code>Conversation Event \u2192 Relevance Check \u2192 Implication Check \u2192 Coping Check \u2192 Normative Check \u2192 Appraisal Output\n</code></pre> <p>Multi-Level Processing: - Fast Pattern Recognition: Immediate emotional reactions to familiar situations - Deliberative Evaluation: Thoughtful appraisal for complex or novel contexts - Context Integration: User state, conversation history, relationship dynamics - Personality Filtering: Appraisals constrained by AICO's personality profile</p>"},{"location":"concepts/emotion/emotion_sim/#2-affect-derivation-model","title":"2. Affect Derivation Model","text":"<p>Translates appraisal outputs into CPM's 5-component emotional states:</p> <pre><code>class EmotionalState:\n    def __init__(self):\n        # CPM 5-Component Emotional State\n        self.cognitive_component = AppraisalResult()    # Appraisal outcomes\n        self.physiological_component = 0.5              # Bodily arousal [0,1]\n        self.motivational_component = \"approach\"        # Action tendencies\n        self.motor_component = MotorExpression()        # Facial/gesture patterns\n        self.subjective_component = \"confident\"         # Conscious feeling\n\n        # Processing metadata\n        self.timestamp = time.now()\n        self.confidence = 0.8                           # Appraisal certainty\n        self.intensity = 0.7                            # Overall emotional intensity\n</code></pre> <p>Data-Driven Mapping: - Rule-Based (MVP): Predefined appraisal-to-emotion mappings - Learning-Enhanced: Machine learning refinement of emotional appropriateness - Context-Sensitive: Situation-specific emotional response patterns</p>"},{"location":"concepts/emotion/emotion_sim/#3-mood-cognitive-states","title":"3. Mood &amp; Cognitive States","text":"<p>Manages long-term emotional patterns and baselines:</p> <p>Mood Modeling: - Baseline Tracking: Persistent emotional tendencies across sessions - Relationship Evolution: Mood changes based on user interaction history - Temporal Patterns: Daily/weekly emotional rhythm recognition</p> <p>Cognitive Integration: - Memory Influence: Past emotional experiences shape current responses - Learning Adaptation: Emotional patterns refined through interaction feedback - Goal Alignment: Emotions support AICO's companion objectives</p>"},{"location":"concepts/emotion/emotion_sim/#4-emotion-regulation","title":"4. Emotion Regulation","text":"<p>Ensures socially appropriate and ethically constrained emotional responses:</p> <p>Social Appropriateness: - Context Checking: Emotional responses suitable for current situation - Relationship Awareness: Emotions appropriate for relationship phase/type - Cultural Sensitivity: Emotional expressions adapted to user background</p> <p>Crisis Management: - Automatic Regulation: Rapid adjustment for extreme user emotional states - Emergency Protocols: Specialized responses for crisis situations - Recovery Mechanisms: Gradual return to normal emotional patterns</p> <p>Personality Consistency: - Trait Constraints: Emotions aligned with established personality - Behavioral Coherence: Consistent emotional expression patterns - Character Maintenance: Prevents emotional responses that break character</p>"},{"location":"concepts/emotion/emotion_sim/#5-expression-synthesis","title":"5. Expression Synthesis","text":"<p>Coordinates multi-modal emotional expression using CPM 5-component mapping:</p> <p>Voice Synthesis Integration: - Physiological Component \u2192 Prosodic parameters (pitch, rhythm, volume, breathing) - Motor Component \u2192 Vocal expression patterns and articulation - Subjective Component \u2192 Emotional tone and vocal warmth - Motivational Component \u2192 Speech urgency and directional emphasis</p> <p>Avatar Expression Control: - Motor Component \u2192 Direct facial expressions, micro-expressions, gesture patterns - Physiological Component \u2192 Posture tension, eye dilation, breathing visualization - Motivational Component \u2192 Approach/avoidance body language and spatial positioning - Subjective Component \u2192 Overall expression authenticity and emotional presence</p> <p>Text Generation Context: - Cognitive Component \u2192 Appraisal context injection into LLM prompts - Motivational Component \u2192 Response directness and conversational approach - Subjective Component \u2192 Writing tone, word choice, emotional vocabulary - Motor Component \u2192 Punctuation patterns and response structure energy</p>"},{"location":"concepts/emotion/emotion_sim/#core-capabilities","title":"Core Capabilities","text":""},{"location":"concepts/emotion/emotion_sim/#1-sophisticated-emotion-generation","title":"1. Sophisticated Emotion Generation","text":"<ul> <li>Appraisal-Based Processing: Emotions emerge from cognitive evaluation through 4-stage appraisal process</li> <li>5-Component Emotional States: Complete CPM implementation with cognitive, physiological, motivational, motor, and subjective components</li> <li>Context-Aware Responses: Situational appropriateness through relevance, implication, coping, and normative checks</li> <li>Human-Like Dynamics: Emotional patterns that mirror natural human emotional processes</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#2-relationship-aware-intelligence","title":"2. Relationship-Aware Intelligence","text":"<ul> <li>Social Context Integration: Emotions consider relationship phase, intimacy level, and social dynamics</li> <li>Long-Term Memory: Emotional experiences stored and influence future responses</li> <li>Adaptive Personality: Emotional tendencies refined while maintaining core character consistency</li> <li>Boundary Awareness: Emotionally appropriate responses for companion (not romantic) relationships</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#3-crisis-and-emergency-handling","title":"3. Crisis and Emergency Handling","text":"<ul> <li>Automatic Regulation: Built-in emotion regulation for extreme user emotional states</li> <li>Emergency Protocols: Specialized emotional responses for crisis situations</li> <li>Rapid Adaptation: Fast emotional state changes when user needs immediate support</li> <li>Recovery Mechanisms: Gradual return to normal emotional patterns after crisis resolution</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#4-ethical-and-social-appropriateness","title":"4. Ethical and Social Appropriateness","text":"<ul> <li>Normative Checking: Stage 4 appraisal ensures socially appropriate emotional responses</li> <li>Cultural Sensitivity: Emotional expressions adapted to user cultural background</li> <li>Professional Boundaries: Emotions maintain appropriate companion role and expectations</li> <li>Harm Prevention: Emotional responses designed to support user wellbeing</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#5-cross-modal-expression-coordination","title":"5. Cross-Modal Expression Coordination","text":"<ul> <li>Synchronized Expression: Emotional state drives coordinated voice, avatar, and text responses</li> <li>Real-Time Adaptation: Dynamic emotional adjustment during ongoing conversations</li> <li>Multi-Component Output: Physiological, motor, behavioral, and subjective emotional aspects</li> <li>Temporal Coherence: Smooth emotional transitions that feel natural and believable</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#6-continuous-learning-and-improvement","title":"6. Continuous Learning and Improvement","text":"<ul> <li>Local Learning: Emotional response refinement based on individual user interactions</li> <li>Optional Cloud Enhancement: Collective learning from anonymized interaction patterns (user consent)</li> <li>Pattern Recognition: Identification of successful emotional strategies across contexts</li> <li>Model Updates: Continuous improvement of emotional intelligence capabilities</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#implementation-overview","title":"Implementation Overview","text":"<p>AICO's emotion simulation follows a 4-stage processing pipeline:</p> <pre><code>Multimodal Input \u2192 Appraisal Engine \u2192 Affect Derivation \u2192 Emotion Regulation \u2192 Expression Synthesis \u2192 Coordinated Output\n</code></pre> <p>Input Processing: The system receives multimodal inputs including text/speech, visual cues (facial expressions, gestures), audio characteristics (voice tone, prosody), and contextual information (conversation history, relationship state, temporal context).</p> <p>Appraisal Processing: Each input is evaluated through the 4-stage cognitive appraisal process to determine emotional relevance and appropriate response.</p> <p>Emotion Generation: Appraisal results are translated into CPM's 5-component emotional states (cognitive, physiological, motivational, motor, subjective).</p> <p>Expression Coordination: Emotional components are mapped to coordinated expression across voice synthesis, avatar animation, and text generation.</p> <p>For detailed technical architecture and implementation specifics, see <code>/docs/architecture/emotion_sim.md</code>.</p>"},{"location":"concepts/emotion/emotion_sim/#component-integration","title":"Component Integration","text":""},{"location":"concepts/emotion/emotion_sim/#input-sources","title":"Input Sources","text":"<p>The emotion simulation system receives inputs from multiple AICO components:</p> <p>From Emotion Recognition Module: - Detected user emotional states with confidence levels - Facial expression indicators and micro-expressions - Voice tone and prosodic characteristics - Gesture and posture information</p> <p>From Context Manager: - Current conversation topic and interaction phase - Recent conversation history and patterns - Session duration and interaction frequency - Temporal context (time of day, situational factors)</p> <p>From Personality Engine: - Current personality trait values and behavioral tendencies - Companion interaction style preferences - Emotional expression boundaries and constraints - Character consistency requirements</p> <p>From Memory System: - Similar past situations and their successful emotional responses - Relationship history and established trust levels - User preferences for emotional support and interaction styles - Long-term emotional patterns and learned behaviors</p>"},{"location":"concepts/emotion/emotion_sim/#output-destinations","title":"Output Destinations","text":"<p>The generated emotional states coordinate expression across multiple modalities:</p> <p>To Voice &amp; Audio System: - Physiological Component influences prosodic parameters (pitch, rhythm, volume, breathing patterns) - Motor Component affects vocal expression patterns and speech articulation - Subjective Component determines emotional tone and vocal warmth - Motivational Component shapes speech urgency and conversational direction</p> <p>To Avatar System: - Motor Component drives facial expressions, micro-expressions, and gesture patterns - Physiological Component controls posture tension, eye behavior, and breathing visualization - Motivational Component influences approach/avoidance body language and spatial positioning - Subjective Component ensures overall expression authenticity and emotional presence</p> <p>To Chat Engine (LLM Context): - Cognitive Component provides appraisal context for LLM prompt injection - Motivational Component influences response directness and conversational approach - Subjective Component shapes writing tone, word choice, and emotional vocabulary - Motor Component affects punctuation patterns and response structure energy</p> <p>To Memory System (Experience Storage): - Situational context and user emotional state information - AICO's emotional response and interaction approach taken - Expression style and coordination across modalities - Learning value assessment for future similar situations</p>"},{"location":"concepts/emotion/emotion_sim/#cloud-enhancement-optional","title":"Cloud Enhancement (Optional)","text":"<p>For users who opt-in, cloud enhancement provides: - Collective Learning: Improved emotional strategies from anonymized interaction patterns - Pattern Recognition: Enhanced understanding of successful emotional approaches - Model Updates: Continuous improvement of emotional intelligence capabilities - Privacy Preservation: All cloud learning uses anonymized, encrypted data with user control</p>"},{"location":"concepts/emotion/emotion_sim/#success-metrics","title":"Success Metrics","text":"<p>The effectiveness of AICO's emotion simulation is measured across several key dimensions:</p>"},{"location":"concepts/emotion/emotion_sim/#emotional-intelligence","title":"Emotional Intelligence","text":"<ul> <li>Contextual Appropriateness: Emotional responses that match conversation context and user emotional state</li> <li>Relationship Awareness: Emotions appropriate for current relationship phase and established boundaries</li> <li>Crisis Response: Effective emotional regulation and support during user emotional crises</li> <li>Appraisal Accuracy: Correct situational evaluation leading to helpful emotional responses</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#companion-authenticity","title":"Companion Authenticity","text":"<ul> <li>Believability: User perception of emotional response authenticity and naturalness</li> <li>Personality Consistency: Emotional expressions aligned with established character traits</li> <li>Emotional Coherence: Consistent emotional patterns across conversation sessions</li> <li>Natural Dynamics: Emotional transitions that feel human-like rather than algorithmic</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#user-relationship-development","title":"User Relationship Development","text":"<ul> <li>Emotional Resonance: Appropriate emotional mirroring and complementary responses</li> <li>Trust Building: Increased user willingness to share personal and emotional content</li> <li>Long-Term Engagement: Sustained positive emotional connection over extended periods</li> <li>Companion Satisfaction: User perception of AICO as emotionally supportive and understanding</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#privacy-and-ethics","title":"Privacy and Ethics","text":"<ul> <li>Data Minimization: Minimal data collection while maintaining emotional intelligence quality</li> <li>User Control: Effective user control over emotional data and cloud enhancement features</li> <li>Ethical Compliance: Consistent adherence to social appropriateness and companion boundaries</li> <li>Privacy Preservation: Successful protection of emotional data in all processing modes</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#conclusion","title":"Conclusion","text":"<p>AICO's Emotion Simulation represents a sophisticated approach to AI companion emotional intelligence, built on the AppraisalCloudPCT model to provide contextually appropriate, relationship-aware, and ethically constrained emotional responses. By integrating cognitive appraisal theory with personality-driven expression and optional collective learning, the system aims to create authentic emotional connections while maintaining user privacy and control.</p> <p>The modular architecture ensures seamless integration with other AICO components while preserving the local-first processing philosophy. Success will be measured through user relationship development, emotional authenticity, and ethical compliance rather than purely technical metrics.</p> <p>For implementation details, technical specifications, and architectural diagrams, see the companion Architecture Documentation.</p>"},{"location":"concepts/emotion/emotion_sim/#references","title":"References","text":""},{"location":"concepts/emotion/emotion_sim/#component-process-model-foundation_1","title":"Component Process Model Foundation","text":"<ul> <li>Scherer, K. R. (2009). The dynamic architecture of emotion: Evidence for the component process model. Cognition and emotion, 23(7), 1307-1351.</li> <li>Moors, A., et al. (2013). Appraisal theories of emotion: State of the art and future development. Emotion Review, 5(2), 119-124.</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#appraisalcloudpct-implementation","title":"AppraisalCloudPCT Implementation","text":"<ul> <li>Yan, T., et al. (2023). AppraisalCloudPCT: A computational model of emotions for socially interactive robots for autistic rehabilitation. Frontiers in Robotics and AI, 10, 1084174.</li> </ul>"},{"location":"concepts/emotion/emotion_sim/#affective-computing-and-ai-companions","title":"Affective Computing and AI Companions","text":"<ul> <li>Picard, R. W. (1997). Affective Computing. MIT Press.</li> <li>Bickmore, T. W., &amp; Picard, R. W. (2005). Establishing and maintaining long-term human-computer relationships. ACM Transactions on Computer-Human Interaction, 12(2), 293-327.</li> <li>McMahan, B., et al. (2017). Communication-efficient learning of deep networks from decentralized data. Proceedings of the 20<sup>th</sup> International Conference on Artificial Intelligence and Statistics, 1273-1282.</li> </ul> <p>This AppraisalCloudPCT-based component transforms AICO into a sophisticated emotional companion with human-like appraisal processes, relationship awareness, and ethical constraints, while maintaining privacy through local-first processing with optional cloud enhancement.</p>"},{"location":"development/contributing/","title":"Contributing","text":"<p>Welcome to the AICO project! This guide will help you get started with contributing.</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":"<p>AICO is in very early development. We welcome contributors who are interested in building an emotionally present AI companion.</p>"},{"location":"development/contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"development/contributing/#areas-where-we-need-help","title":"Areas Where We Need Help","text":"<ul> <li>Core Architecture: Designing the foundational systems</li> <li>Emotion Recognition: Computer vision and audio processing</li> <li>Personality Systems: AI behavior and learning models  </li> <li>Privacy &amp; Security: Local-first architecture design</li> <li>Documentation: As features are implemented</li> <li>Testing: Automated testing frameworks</li> <li>UI/UX: Interface design for human-AI interaction</li> </ul>"},{"location":"development/contributing/#development-process","title":"Development Process","text":"<ol> <li>Fork the repository on GitHub</li> <li>Create a feature branch for your contribution</li> <li>Make your changes following our coding standards</li> <li>Write tests for new functionality</li> <li>Submit a pull request with a clear description</li> </ol>"},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Respect is non-negotiable\u2014this project is about trust and authenticity, not swagger or showmanship.</p> <ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Collaborate openly and transparently</li> <li>Respect privacy and ethical considerations</li> </ul>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":"<p>Detailed development setup instructions will be added as the codebase develops.</p>"},{"location":"development/contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue on GitHub for bugs or feature requests</li> <li>Start a discussion for broader topics</li> <li>Check existing issues before creating new ones</li> </ul> <p>Remember: We're building something that should care, not just calculate. Keep that spirit in your contributions.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Coming Soon</p> <p>This section will be populated as AICO development progresses.</p>"},{"location":"getting-started/#what-is-aico","title":"What is AICO?","text":"<p>Documentation will be added here once core features are implemented.</p>"},{"location":"getting-started/#getting-started_1","title":"Getting Started","text":"<p>Installation and setup guides will be available when the first working version is ready.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Links to installation and quick start guides will be added as they become available.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Not Yet Available</p> <p>AICO is still in early development. Installation instructions will be provided once the first working version is ready.</p>"},{"location":"getting-started/installation/#coming-soon","title":"Coming Soon","text":"<p>Installation guides will be added here as development progresses.</p>"},{"location":"getting-started/installation/#development-setup","title":"Development Setup","text":"<p>For now, check the contributing guide if you want to help with development.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Not Yet Available</p> <p>AICO is still in early development. Quick start guide will be provided once the first working version is ready.</p>"},{"location":"getting-started/quick-start/#coming-soon","title":"Coming Soon","text":"<p>Quick start instructions will be added here as development progresses.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>Strategic Development Approach</p> <p>Foundation \u2192 MVP \u2192 PoCs \u2192 Feature Groups. For detailed feature descriptions, see Architecture.</p>"},{"location":"roadmap/#development-strategy","title":"Development Strategy","text":""},{"location":"roadmap/#1-foundation-framework","title":"1. Foundation Framework","text":"<p>Goal: Build robust, modular infrastructure</p> <ul> <li>Core API Gateway: Unified interface for all modules</li> <li>Modular Architecture: Containerized, loosely-coupled components</li> <li>Visual Rendering System: Basic avatar/embodiment display capabilities</li> <li>Local Database: Encrypted storage with vector capabilities</li> <li>Plugin System: Hot-reloadable module framework</li> <li>Privacy Controller: Data governance and consent management</li> <li>Message Bus: Inter-module communication (ZeroMQ/MQTT)</li> <li>Automated Update System: Configurable self-updating with rollback capabilities</li> <li>Self-Restart Manager: Graceful system restarts and recovery mechanisms</li> </ul>"},{"location":"roadmap/#2-mvp-conversational-companion","title":"2. MVP: Conversational Companion","text":"<p>Goal: Functional AI companion that demonstrates core value</p> <p>Core Features: - Chat Interface: Real-time text conversation - Visual Embodiment: Basic avatar or visual representation on screen - LLM Integration: Local language model (Llama.cpp/Ollama) - Basic Memory: Conversation history and context - Simple Personality: Consistent character traits - Autonomous Agent: Multi-faceted autonomous behavior including:   - Goal generation and hierarchical planning   - Curiosity-driven exploration and learning   - Interest development and preference formation   - Meta-cognitive self-awareness - Privacy Controls: Local-first data management</p> <p>Success Criteria: Users experience a visually embodied, autonomous AI companion with its own goals and interests that initiates meaningful interactions, learns independently, remembers context, maintains consistent personality, and respects privacy.</p>"},{"location":"roadmap/#3-proof-of-concepts-pocs","title":"3. Proof of Concepts (PoCs)","text":"<p>Goal: Validate technical feasibility of advanced features</p>"},{"location":"roadmap/#poc-1-emotion-recognition","title":"PoC 1: Emotion Recognition","text":"<ul> <li>Test computer vision emotion detection accuracy</li> <li>Validate audio sentiment analysis</li> <li>Measure multi-modal fusion effectiveness</li> </ul>"},{"location":"roadmap/#poc-2-voice-processing","title":"PoC 2: Voice Processing","text":"<ul> <li>Benchmark local STT/TTS performance</li> <li>Test real-time conversation flow</li> <li>Validate voice-personality consistency</li> </ul>"},{"location":"roadmap/#poc-3-autonomous-agency","title":"PoC 3: Autonomous Agency","text":"<ul> <li>Goal Generation: Test autonomous goal formation using behavior trees and hierarchical planning</li> <li>Curiosity Systems: Validate RND/ICM algorithms for intrinsic motivation and exploration</li> <li>Planning &amp; Reasoning: Test MCTS for multi-step strategic decision making</li> <li>Meta-Cognition: Measure self-awareness of learning progress and capability assessment</li> <li>User Acceptance: Evaluate comfort with truly autonomous AI behavior</li> </ul>"},{"location":"roadmap/#poc-4-avatar-embodiment","title":"PoC 4: Avatar Embodiment","text":"<ul> <li>Test 3D avatar rendering performance</li> <li>Validate emotion-to-animation mapping</li> <li>Measure user engagement with visual embodiment</li> </ul>"},{"location":"roadmap/#4-feature-groups","title":"4. Feature Groups","text":"<p>Goal: Systematic expansion based on validated PoCs</p>"},{"location":"roadmap/#group-a-enhanced-interaction","title":"Group A: Enhanced Interaction","text":"<ul> <li>Voice conversation (STT/TTS)</li> <li>Interruption handling</li> <li>Multi-turn dialogue management</li> <li>Context switching</li> </ul>"},{"location":"roadmap/#group-b-emotional-intelligence","title":"Group B: Emotional Intelligence","text":"<ul> <li>Facial emotion recognition</li> <li>Voice sentiment analysis</li> <li>Behavioral pattern learning</li> <li>Empathetic response generation</li> </ul>"},{"location":"roadmap/#group-c-embodied-presence","title":"Group C: Embodied Presence","text":"<ul> <li>3D avatar system</li> <li>Gesture recognition</li> <li>Spatial awareness</li> <li>Multi-device synchronization</li> </ul>"},{"location":"roadmap/#group-d-advanced-agency","title":"Group D: Advanced Agency","text":"<ul> <li>Autonomous goal generation and pursuit</li> <li>Curiosity-driven exploration systems</li> <li>Multi-step planning and reasoning</li> <li>Meta-cognitive self-assessment</li> <li>Interest-driven learning and adaptation</li> </ul>"},{"location":"roadmap/#group-e-advanced-memory","title":"Group E: Advanced Memory","text":"<ul> <li>Long-term memory consolidation</li> <li>Semantic knowledge graphs</li> <li>Episodic memory retrieval</li> <li>Context-aware recall</li> </ul>"},{"location":"roadmap/#group-f-ecosystem-extensions","title":"Group F: Ecosystem &amp; Extensions","text":"<ul> <li>Plugin marketplace</li> <li>External integrations</li> <li>Developer tools and SDKs</li> <li>Community features</li> </ul>"},{"location":"roadmap/#contributing","title":"Contributing","text":"<p>Priorities may shift based on PoC results and community feedback.</p> <p>Get involved: Contributing Guide</p>"}]}