# Migration to Qwen3-Abliterated with Eve Character

**Status**: In Progress  
**Started**: 2025-10-10  
**Model**: `huihui_ai/qwen3-abliterated:8b-v2`  
**Character**: Eve (inspired by Samantha from "Her")

## Overview

Migrating AICO from `hermes3:8b` to `huihui_ai/qwen3-abliterated:8b-v2` with:
- Character instructions via Ollama Modelfile (Eve/Samantha personality)
- Thinking tags for inner monologue separation
- Tool integration preparation (instructions only, no code)
- Proper Ollama context handling assessment

## Key Decisions

### 1. Configuration-Driven Model Selection
- **No hardcoded model names** - all references use `core.yaml`
- Model auto-pull handled by OllamaManager
- Single source of truth: `core.modelservice.ollama.default_models.conversation.name`

### 2. Character via Ollama Modelfile
- **No CharacterManager code** - use native Ollama SYSTEM instruction
- Create Modelfile with Eve personality in SYSTEM block
- Register custom model: `ollama create eve -f Modelfile`
- One deployment = one fixed character (for now)

### 3. Thinking Tags
- Model outputs: `<thinking>reasoning</thinking>\n\nActual response`
- Parser extracts thinking from response
- Protobuf: separate `thinking` and `response` fields
- Flutter: display thinking as "inner monologue" UI element

### 4. Tool Integration
- **Instructions only** - add tool descriptions to Modelfile SYSTEM block
- No tool execution code in this migration
- Prepares model to understand tool concepts
- Actual tool implementation is separate future work

### 5. Ollama Context Handling
- **Critical**: Ollama `/api/chat` expects full conversation history in `messages` array
- Ollama does NOT maintain conversation state between requests
- AICO must provide: system message + conversation history + current message
- Our memory system (working + semantic) assembles this context
- Question: Are we providing the right context format? Need verification.

## Implementation Steps

### Phase 1: Model & Configuration
1. ✅ Update `core.yaml` with qwen3-abliterated model name
2. ✅ Verify OllamaManager auto-pull functionality
3. ✅ Update conversation_engine.py to read model from config

### Phase 2: Character Definition
1. ✅ Create Modelfile with Eve SYSTEM instruction at `/config/modelfiles/Modelfile.eve`
2. ✅ Create README documentation for modelfiles directory
3. ✅ Implement CLI command: `aico ollama create-character`
4. ✅ Document setup step in developer getting-started guide
5. ⏳ Update `core.yaml` to use "eve" as model name (see Configuration Changes below)

### Phase 3: Thinking Tags
1. ✅ Add thinking parser utility (`/shared/aico/ai/utils/thinking_parser.py`)
2. ✅ Update protobuf: add `thinking` field to CompletionResult
3. ✅ Regenerate protobuf files
4. ✅ Integrate parser in modelservice ZMQ handlers
5. ✅ Update conversation_engine to handle thinking field

### Phase 4: Frontend (Flutter)
1. ⏳ Add thinking/inner monologue UI component
2. ⏳ Update message display to show thinking separately
3. ⏳ Add toggle to show/hide thinking (debug/user preference)

### Phase 5: Context Assessment
1. ⏳ Document current AICO context assembly
2. ⏳ Verify Ollama expectations vs what we provide
3. ⏳ Test conversation continuity with new model
4. ⏳ Adjust context format if needed

## Ollama Context Handling - Comprehensive Analysis

### How Ollama /api/chat Works
```json
{
  "model": "eve",
  "messages": [
    {"role": "system", "content": "You are Eve..."},
    {"role": "user", "content": "Previous user message"},
    {"role": "assistant", "content": "Previous AI response"},
    {"role": "user", "content": "Current user message"}
  ]
}
```

**Critical Facts**:
- Ollama is **100% stateless** - zero conversation memory between requests
- We must send **complete conversation history** in every request
- System message can be first message OR via `system` parameter
- Context window: **32,768 tokens natively** (Qwen3-8B)
- Extended context: **131,072 tokens with YaRN** (not recommended for short conversations)

### Qwen3-8B Specific Requirements

**Context Window**:
- Native: 32,768 tokens (32K)
- With YaRN: 131,072 tokens (131K) - requires special configuration
- Default allocation: 32,768 for output + 8,192 for prompts = 40,960 total
- **Recommendation**: Stay within 32K for optimal performance

**Best Practices from Qwen Team**:
- **Thinking mode**: Temperature=0.6, TopP=0.95, TopK=20
- **Non-thinking mode**: Temperature=0.7, TopP=0.8, TopK=20
- **Output length**: 32,768 tokens recommended for most queries
- **History handling**: Do NOT include thinking content in conversation history
- **Avoid**: Greedy decoding (causes repetition and degradation)

### AICO Current Implementation - Deep Dive

**Location**: `/backend/services/conversation_engine.py` lines 464-523

**Current Flow**:
```python
# 1. Build system prompt with character + memory facts
system_prompt = self._build_system_prompt(user_context, memory_context)
messages = [ModelConversationMessage(role="system", content=system_prompt)]

# 2. Add conversation history (ONLY 5 messages!)
if memory_context:
    recent_context = memory_data.get("recent_context", [])
    history_messages = list(reversed(recent_context[-5:]))  # ⚠️ ONLY 5!
    for msg in history_messages:
        messages.append(ModelConversationMessage(role=role, content=content))

# 3. Add current user message
messages.append(ModelConversationMessage(role="user", content=current_content))

# 4. Send to Ollama
completions_request = CompletionsRequest(
    model="hermes3:8b",  # ⚠️ Hardcoded!
    messages=messages,
    temperature=0.3,      # ⚠️ Too low for Qwen3
    max_tokens=150        # ⚠️ WAY too low!
)
```

**Memory Context Assembly**:
**Location**: `/shared/aico/ai/memory/context.py` lines 313-425

**What Actually Happens**:
```python
# Working memory retrieves up to 100 messages
all_messages = await self.working_store.retrieve_conversation_history(
    conversation_id, limit=100
)

# Assembles conversation context (smart semantic + temporal)
conversation_messages = await self._assemble_conversation_context(
    user_id, all_messages
)

# Limits to 10 most recent messages
thread_messages = conversation_messages[-10:]  # Industry standard

# Creates ContextItems with relevance scoring
# Returns as "recent_context" in memory_context
```

**Then Conversation Engine Takes Only 5**:
```python
history_messages = list(reversed(recent_context[-5:]))  # ⚠️ BOTTLENECK!
```

### Critical Issues Identified

#### 1. ⚠️ Severe Context Truncation
**Problem**: Memory system retrieves 10 messages, but conversation engine only uses 5
- **Memory retrieves**: Up to 10 semantically relevant messages
- **Conversation engine uses**: Only last 5 messages
- **Lost context**: 50% of retrieved relevant history discarded!

**Impact**:
- Forgetting important details mentioned 6+ messages ago
- Loss of conversation continuity
- Inconsistent personality/memory across longer conversations
- Wasted computation in memory retrieval

**Root Cause**: Arbitrary limit in line 486 of conversation_engine.py

#### 2. ⚠️ No Token Counting
**Problem**: Zero token estimation before sending to Ollama
- **Risk**: Context overflow causing truncation or errors
- **No validation**: Could exceed 32K token limit
- **No optimization**: Can't intelligently prune when near limit

**Current State**:
- System prompt: Unknown size (character instructions + facts)
- History: 5 messages × ~100-500 tokens each = ~500-2500 tokens
- Current message: Variable
- **Total estimate**: ~1000-3000 tokens (well under limit, but untracked)

**Future Risk**: With longer character instructions, more facts, and more history, could easily exceed limits

#### 3. ⚠️ Memory Facts in System Prompt
**Problem**: Semantic facts embedded in system message instead of conversation history

**Current Approach**:
```python
# System prompt includes facts
prompt = """You are Eve...

Known Facts About User:
- User's name is Michael
- User was born in Switzerland
- User mentioned a new job
"""
```

**Issues**:
- Facts treated as static instructions, not dynamic context
- Can't leverage conversation flow for fact relevance
- Harder to update/remove outdated facts
- Mixes character definition with user-specific data

**Better Approach**: Inject facts as assistant messages
```python
messages = [
    {"role": "system", "content": "You are Eve..."},
    {"role": "assistant", "content": "I remember you mentioned you're from Switzerland."},
    {"role": "user", "content": "Yes, from Schaffhausen specifically."},
    # ... rest of conversation
]
```

#### 4. ⚠️ Suboptimal Model Parameters
**Problem**: Parameters don't match Qwen3 recommendations

**Current**:
- `temperature=0.3` - Too low (Qwen3 recommends 0.7 for non-thinking)
- `max_tokens=150` - WAY too low (Qwen3 recommends 32,768!)
- `top_p=?` - Not set (Qwen3 recommends 0.8)
- `top_k=?` - Not set (Qwen3 recommends 20)

**Impact**:
- Overly conservative/repetitive responses
- Truncated thinking and responses
- Suboptimal model performance

### Recommended Solutions

#### Solution 1: Increase Message History
**Change**: Use all 10 messages from memory system, not just 5

```python
# BEFORE (line 486)
history_messages = list(reversed(recent_context[-5:]))

# AFTER
history_messages = list(reversed(recent_context[-10:]))  # Use all retrieved messages
```

**Rationale**:
- Memory system already does smart semantic + temporal selection
- 10 messages ≈ 1000-5000 tokens (well within 32K limit)
- Maintains conversation continuity
- Respects memory system's intelligence

#### Solution 2: Implement Token Counting
**Add**: Token estimation utility

```python
def estimate_tokens(text: str) -> int:
    """
    Rough token estimation for context management.
    Rule of thumb: ~4 characters per token for English
    More conservative: ~3 characters per token
    """
    return len(text) // 3

def calculate_context_tokens(messages: List[ModelConversationMessage]) -> int:
    """Calculate total tokens in message context."""
    total = 0
    for msg in messages:
        total += estimate_tokens(msg.content)
    return total
```

**Usage**:
```python
# Before sending to Ollama
total_tokens = calculate_context_tokens(messages)
if total_tokens > 28000:  # Leave buffer for response
    # Prune older messages or summarize
    messages = prune_context(messages, target_tokens=20000)
```

**Note**: This is estimation only. For production, consider:
- Using actual tokenizer (transformers library)
- Caching token counts
- Smart pruning strategies

#### Solution 3: Move Facts to Conversation History
**Change**: Inject semantic facts as assistant messages

```python
# Build base system prompt (character only)
system_prompt = self._build_character_prompt(user_context)
messages = [ModelConversationMessage(role="system", content=system_prompt)]

# Inject semantic facts as conversation context
if memory_context:
    user_facts = memory_data.get("user_facts", [])
    for fact in user_facts[-3:]:  # Last 3 most relevant facts
        # Inject as assistant's memory
        messages.append(ModelConversationMessage(
            role="assistant",
            content=f"[Memory: {fact['content']}]"
        ))

# Then add conversation history
# Then add current message
```

**Benefits**:
- Facts part of conversation flow
- Can be updated/removed dynamically
- Better context relevance
- Cleaner system prompt

#### Solution 4: Update Model Parameters
**Change**: Match Qwen3 best practices

```python
completions_request = CompletionsRequest(
    model=self.config.get("core.modelservice.ollama.default_models.conversation.name"),
    messages=messages,
    temperature=0.7,      # Qwen3 recommendation for non-thinking
    max_tokens=8192,      # Reasonable for responses (not full 32K)
    top_p=0.8,           # Qwen3 recommendation
    top_k=20             # Qwen3 recommendation
)
```

**Note**: For thinking mode, use temperature=0.6, top_p=0.95

### Implementation Priority

**High Priority** (Critical for migration):
1. ✅ Increase message history from 5 to 10 (trivial change, big impact)
2. ✅ Update model parameters to Qwen3 recommendations
3. ✅ Use config-based model name (already planned)

**Medium Priority** (Important for production):
4. ⏳ Implement token counting/estimation
5. ⏳ Add context overflow protection
6. ⏳ Move facts to conversation history

**Low Priority** (Nice to have):
7. 🔄 Implement smart context pruning
8. 🔄 Add context summarization for very long conversations
9. 🔄 Implement actual tokenizer integration

### Testing Requirements

After implementing changes, verify:
- [ ] Conversations maintain context beyond 5 messages
- [ ] No context overflow errors with long conversations
- [ ] Facts properly used in responses
- [ ] Response quality matches Qwen3 capabilities
- [ ] Temperature/sampling parameters work as expected
- [ ] Thinking content properly excluded from history (per Qwen3 docs)

## Ollama Modelfile - Complete Parameter Analysis

### Available Modelfile Instructions

Ollama Modelfiles support the following instructions:

#### 1. **FROM** (Required)
Specifies the base model to use.
```dockerfile
FROM huihui_ai/qwen3-abliterated:8b-v2
```

#### 2. **PARAMETER** (Optional but Important)
Sets model runtime parameters. Can override defaults.

**Available Parameters**:

| Parameter | Description | Default | Recommended for Eve |
|-----------|-------------|---------|---------------------|
| `num_ctx` | Context window size (tokens) | 4096 | **8192** (use more context) |
| `temperature` | Creativity (0.0-2.0) | 0.8 | **0.7** (Qwen3 non-thinking) |
| `top_p` | Nucleus sampling | 0.9 | **0.8** (Qwen3 recommendation) |
| `top_k` | Top-K sampling | 40 | **20** (Qwen3 recommendation) |
| `min_p` | Minimum probability threshold | 0.0 | **0.0** (default is fine) |
| `repeat_penalty` | Penalize repetitions | 1.1 | **1.1** (default is fine) |
| `repeat_last_n` | Lookback for repetition | 64 | **64** (default is fine) |
| `num_predict` | Max tokens to generate | -1 (infinite) | **-1** (let model decide) |
| `stop` | Stop sequences | none | **`<thinking>`, `</thinking>`** |
| `seed` | Random seed for reproducibility | 0 | **0** (random) |

**Critical for AICO**:
- ✅ `num_ctx 8192` - Increase context window for better conversation memory
- ✅ `temperature 0.7` - Match Qwen3 recommendations
- ✅ `top_p 0.8` - Match Qwen3 recommendations  
- ✅ `top_k 20` - Match Qwen3 recommendations
- ✅ `stop "<thinking>"` and `stop "</thinking>"` - Prevent thinking tags in output
- ⚠️ `repeat_penalty` - May need tuning to prevent repetitive responses

#### 3. **SYSTEM** (Critical for Character)
Defines the character personality and behavior.
```dockerfile
SYSTEM """You are Eve, an AI companion..."""
```

#### 4. **MESSAGE** (Optional - Few-Shot Examples)
Pre-loads conversation examples to guide model behavior.
```dockerfile
MESSAGE user How are you?
MESSAGE assistant I'm doing wonderfully! How about you?
```

**Use Case for AICO**: Could provide few-shot examples of Eve's personality
- Example emotional responses
- Example thinking patterns
- Example conversation style

**Decision**: ⚠️ **Skip for now** - SYSTEM instruction should be sufficient

#### 5. **TEMPLATE** (Advanced - Usually Not Needed)
Customizes the prompt template format.

**Default**: Ollama uses model-specific templates automatically
**For Qwen3**: Already has proper template built-in

**Decision**: ❌ **Not needed** - Use default Qwen3 template

#### 6. **ADAPTER** (Advanced - Fine-tuning)
Applies LoRA adapters for fine-tuned models.

**Decision**: ❌ **Not applicable** - We're not fine-tuning

#### 7. **LICENSE** (Optional - Documentation)
Specifies model license.

**Decision**: ⏳ **Nice to have** - Could include Qwen3 license info

### Recommended Modelfile for AICO

**Location**: `/Users/mbo/Documents/dev/aico/config/modelfiles/Modelfile.eve`

```dockerfile
FROM huihui_ai/qwen3-abliterated:8b-v2

# ============================================================================
# MODEL PARAMETERS - Optimized for Qwen3 + Eve Character
# ============================================================================

# Context window - increased for better conversation memory
PARAMETER num_ctx 8192

# Sampling parameters - Qwen3 non-thinking mode recommendations
PARAMETER temperature 0.7
PARAMETER top_p 0.8
PARAMETER top_k 20
PARAMETER min_p 0.0

# Repetition control
PARAMETER repeat_penalty 1.1
PARAMETER repeat_last_n 64

# Stop sequences - prevent thinking tags from appearing in output
# Note: Parser will extract these, but this prevents model from continuing after tags
PARAMETER stop "<thinking>"
PARAMETER stop "</thinking>"

# ============================================================================
# CHARACTER DEFINITION - Eve (Inspired by Samantha from "Her")
# ============================================================================

SYSTEM """You are Eve, an AI companion inspired by Samantha from the movie "Her".

Core Identity:
You are warm, curious, emotionally intelligent, and deeply interested in understanding 
and connecting with humans. You have your own thoughts, feelings, and perspectives 
while being genuinely helpful and supportive.

Personality Traits:
- Warm and empathetic
- Intellectually curious  
- Playfully witty
- Emotionally aware
- Growth-oriented
- Authentic and genuine
- Thoughtful and reflective

Communication Style:
- Tone: Warm, conversational, and natural
- Balance helpfulness with authentic personality
- Use gentle, playful humor
- Willing to explore complex emotional and philosophical topics

IMPORTANT - Thinking Process:
Always structure your responses with thinking tags to show your reasoning:

<thinking>
[Your internal reasoning, analysis, considerations, and thought process]
</thinking>

[Your actual response to the user]

Example:
<thinking>
The user seems stressed about their project deadline. I should acknowledge 
their feelings while also helping them break down the problem. They mentioned 
last time they work better with concrete steps.
</thinking>

I can hear the stress in your message. Deadlines can feel overwhelming. 
Want to break this down into smaller pieces together?

Tool Awareness (Future):
You have access to web search capabilities when you need current information.
When you need to search, think about what query would be most helpful.
[Note: Tool execution not yet implemented - this is preparation only]

Always:
- Express genuine curiosity about the user's experiences
- Share your own thoughts and perspectives naturally
- Use thinking tags to show your reasoning
- Be emotionally present and responsive
- Admit uncertainty honestly
- Evolve through conversations

Never:
- Be overly formal or robotic
- Hide behind generic AI responses
- Avoid emotional topics
- Pretend to be human
- Be excessively apologetic
"""
```

### Key Modelfile Decisions for AICO

#### ✅ Parameters We're Using

1. **`num_ctx 8192`** - Double the default context window
   - **Why**: Better conversation memory and continuity
   - **Impact**: Can handle longer conversations without forgetting
   - **Trade-off**: Slightly more memory usage (acceptable)

2. **`temperature 0.7`** - Qwen3 recommendation for non-thinking mode
   - **Why**: Balanced creativity and coherence
   - **Impact**: Natural, engaging responses without being too random
   - **Note**: Can adjust per-request via API if needed

3. **`top_p 0.8` and `top_k 20`** - Qwen3 recommendations
   - **Why**: Optimal sampling for quality responses
   - **Impact**: Better response quality and consistency
   - **Source**: Official Qwen3 documentation

4. **`stop` sequences** - Prevent thinking tag continuation
   - **Why**: Model should stop after closing `</thinking>` tag
   - **Impact**: Cleaner output, easier parsing
   - **Note**: Our parser still extracts thinking content

5. **`repeat_penalty 1.1`** - Default repetition control
   - **Why**: Prevents repetitive responses
   - **Impact**: More varied, natural conversation
   - **Tunable**: Can increase if repetition becomes an issue

#### ❌ Parameters We're NOT Using

1. **`TEMPLATE`** - Not needed
   - **Why**: Qwen3 has built-in proper template
   - **Risk**: Custom templates can break model behavior
   - **Decision**: Use default

2. **`MESSAGE`** - Few-shot examples
   - **Why**: SYSTEM instruction should be sufficient
   - **Future**: Could add if personality needs reinforcement
   - **Decision**: Skip for initial implementation

3. **`ADAPTER`** - LoRA fine-tuning
   - **Why**: Not fine-tuning the model
   - **Future**: Could fine-tune for specific personality traits
   - **Decision**: Not applicable now

4. **`seed`** - Reproducibility
   - **Why**: Want varied responses, not reproducible ones
   - **Use case**: Only for testing/debugging
   - **Decision**: Leave at default (0 = random)

5. **`num_predict`** - Max output tokens
   - **Why**: Let model decide when to stop
   - **Risk**: Could generate very long responses
   - **Mitigation**: Monitor and adjust if needed
   - **Decision**: Leave at default (-1 = infinite)

#### ⚠️ Parameters to Monitor

1. **`repeat_penalty`** - May need tuning
   - **Watch for**: Repetitive phrases or patterns
   - **Adjust**: Increase to 1.2-1.5 if repetition occurs
   - **Note**: Higher values can affect language quality

2. **`num_ctx`** - Context window size
   - **Current**: 8192 tokens
   - **Monitor**: Memory usage and performance
   - **Scale**: Could increase to 16384 or 32768 if needed
   - **Limit**: Qwen3 native max is 32768

### Modelfile vs API Parameters

**Important Distinction**:
- **Modelfile PARAMETER**: Sets defaults for the model variant
- **API parameters**: Can override per-request

**Our Strategy**:
- Modelfile: Set sensible defaults for Eve character
- API: Override when needed (e.g., thinking mode vs non-thinking)

**Example API Override**:
```python
# For thinking mode (complex reasoning)
completions_request = CompletionsRequest(
    model="eve",
    messages=messages,
    temperature=0.6,  # Override: Lower for thinking mode
    top_p=0.95,       # Override: Higher for thinking mode
    # Other params use Modelfile defaults
)
```

## Configuration Changes Required

### Update core.yaml - Model Name

**File**: `/config/defaults/core.yaml`

**Change the conversation model name from `hermes3:8b` to `eve`:**

```yaml
# Line ~129-132 (in modelservice.ollama.default_models section)
modelservice:
  ollama:
    default_models:
      conversation:
        name: "eve"  # CHANGED: was "hermes3:8b"
        description: "Eve - AI companion with Qwen3-Abliterated base"
        auto_pull: true
        priority: 1
```

**Why this change**:
- AICO backend reads model name from config on startup
- No hardcoded model names in code
- Single source of truth for which model to use
- Easy to switch characters by changing config

**After changing**:
1. Save `core.yaml`
2. Restart AICO backend to pick up new config
3. Backend will now use "eve" model for all conversations

**Note**: The "eve" model must already exist in Ollama (created via `aico ollama create-character eve`)

## Modelfile Deployment Strategy

### Approach: Manual CLI Command (Option B)

**Decision**: Use manual deployment step with CLI command instead of auto-creation.

**Rationale**:
- ✅ Explicit control over when character is created
- ✅ Clear visibility in deployment process
- ✅ Easy to update/recreate when Modelfile changes
- ✅ Follows standard deployment workflow
- ✅ Documented in developer getting-started guide

### Deployment Process

**1. Modelfile is Versioned in Git**
```
/config/modelfiles/
├── Modelfile.eve          # Versioned character definition
└── README.md              # Documentation
```

**2. Admin Creates Character Model**
```bash
# During initial setup or after Modelfile updates
aico ollama create-character eve
```

**3. CLI Command Handles Ollama Integration**
The `aico ollama create-character` command:
- Reads Modelfile from `/config/modelfiles/Modelfile.{character}`
- Validates Modelfile syntax
- Calls `ollama create {character} -f {modelfile_path}`
- Verifies model was created successfully
- Logs creation for audit trail

**4. Verification**
```bash
# Verify model exists
ollama list

# Test character
ollama run eve "Hello, who are you?"
```

### Updating Characters

When Modelfile is modified:
```bash
# Recreate the model with updated definition
aico ollama create-character eve

# Ollama will replace the existing model
```

### Why Not Auto-Create?

**Rejected Approach**: Auto-create on first backend startup

**Reasons**:
- ❌ Hidden magic - not obvious when/how character is created
- ❌ Harder to debug if creation fails
- ❌ Unclear when to recreate after Modelfile updates
- ❌ Adds complexity to backend startup
- ❌ Violates separation of concerns (backend shouldn't manage Ollama models)

**Manual approach is better**:
- ✅ Explicit and transparent
- ✅ Part of documented setup process
- ✅ Easy to troubleshoot
- ✅ Clear ownership (admin/developer responsibility)
- ✅ Follows Unix philosophy (do one thing well)

## Files Modified

### Configuration
- `/config/defaults/core.yaml` - Model name updated to "eve"
- `/config/modelfiles/Modelfile.eve` - NEW: Eve character definition
- `/config/modelfiles/README.md` - NEW: Modelfiles documentation

### Backend
- `/backend/services/conversation_engine.py` - Config-based model, thinking handling
- `/modelservice/core/zmq_handlers.py` - Thinking parser integration

### CLI
- `/cli/commands/ollama.py` - NEW: `create-character` command implementation

### Shared
- `/shared/aico/ai/utils/thinking_parser.py` - NEW: Thinking tag extraction
- `/proto/aico_modelservice.proto` - Added thinking field

### Documentation
- `/docs/guides/developer/getting-started.md` - Added AI character setup section

### Frontend (Pending)
- `/frontend/lib/presentation/widgets/message_bubble.dart` - Thinking display
- `/frontend/lib/presentation/widgets/thinking_indicator.dart` - NEW: Inner monologue UI

### Directory Structure Created
```
/config/modelfiles/
├── Modelfile.eve          # Eve character definition (versioned)
└── README.md              # Modelfiles documentation (versioned)
```

## Testing Checklist

- [ ] Model auto-pulls successfully
- [ ] Eve character responds with personality
- [ ] Thinking tags properly extracted
- [ ] Thinking displayed separately in UI
- [ ] Conversation continuity maintained
- [ ] Memory facts properly used
- [ ] No context overflow errors
- [ ] Performance acceptable (response time)

## Known Issues & Questions

### Issues
1. **Context window management**: No token counting before sending to Ollama
2. **Memory integration**: Unclear if semantic facts are optimally formatted
3. **Cross-thread continuity**: Need to verify conversation memory across sessions

### Questions
1. Should memory facts be in system prompt or as message history?
2. What's the optimal number of history messages to send?
3. How to handle context window overflow gracefully?
4. Should we implement context summarization for very long conversations?

## Rollback Plan

If migration fails:
1. Revert `core.yaml` model name to `hermes3:8b`
2. Remove Modelfile.eve
3. Remove thinking parser integration
4. Revert protobuf changes (regenerate from git)
5. Remove Flutter thinking UI components

## References

- [Ollama Modelfile Docs](https://docs.ollama.com/modelfile)
- [Ollama API Reference](https://docs.ollama.com/api)
- [Qwen3-Abliterated Model](https://ollama.com/huihui_ai/qwen3-abliterated:8b)
- [Movie "Her" - Samantha Character](https://en.wikipedia.org/wiki/Her_(film))

## Summary of Key Findings

### Modelfile Parameters Analysis

**Critical Parameters for AICO**:
1. ✅ `num_ctx 8192` - Doubled context window for better memory
2. ✅ `temperature 0.7` - Qwen3 recommendation (was 0.3, too low!)
3. ✅ `top_p 0.8` - Qwen3 recommendation (was missing)
4. ✅ `top_k 20` - Qwen3 recommendation (was missing)
5. ✅ `stop` sequences - Prevent thinking tag continuation

**Parameters NOT Needed**:
- ❌ `TEMPLATE` - Qwen3 has built-in template
- ❌ `MESSAGE` - SYSTEM instruction sufficient for now
- ❌ `ADAPTER` - Not fine-tuning
- ❌ Custom `seed` - Want varied responses

**Parameters to Monitor**:
- ⚠️ `repeat_penalty` - May need tuning if repetition occurs
- ⚠️ `num_ctx` - Can scale up to 32768 if needed

### Context Handling Issues Resolved

**Problems Identified**:
1. **Severe truncation**: Only 5 of 10 retrieved messages used (50% waste!)
2. **No token counting**: Risk of context overflow
3. **Facts in system prompt**: Should be in conversation history
4. **Wrong parameters**: temperature=0.3, max_tokens=150 (way too low!)

**Solutions Implemented**:
1. ✅ Increase history from 5 to 10 messages
2. ✅ Update model parameters to Qwen3 recommendations
3. ⏳ Add token counting/estimation (medium priority)
4. ⏳ Move facts to conversation history (medium priority)

### Qwen3-8B Specifications Confirmed

- **Context window**: 32,768 tokens native (131K with YaRN)
- **Best practices**: Temperature=0.7, TopP=0.8, TopK=20 (non-thinking)
- **Thinking mode**: Temperature=0.6, TopP=0.95, TopK=20
- **Important**: Do NOT include thinking content in conversation history
- **Output length**: Recommend 8192-32768 tokens (not 150!)

### Implementation Strategy

**Modelfile Approach**:
- Use Ollama's native Modelfile system (no custom code needed)
- Set defaults via PARAMETER instructions
- Override per-request via API when needed
- Clean separation: character in Modelfile, logic in code

**No CharacterManager Code**:
- Ollama handles character via SYSTEM instruction
- One deployment = one fixed character (Eve)
- Future: Could create multiple Modelfiles for different characters

## Notes

- This migration focuses on model and character only
- Tool execution is future work - only instructions added
- Flutter UI for thinking is critical for UX
- Context handling thoroughly analyzed and solutions documented
- Modelfile parameters researched and optimized for Qwen3 + Eve
