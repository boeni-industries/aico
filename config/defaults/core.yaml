system:
  name: "AICO"
  environment: "development"
  # Path configuration - uses cross-platform path resolution
  paths:
    directory_mode: "auto"  # auto = OS-appropriate dirs, current = ./data, or explicit path
    data_subdirectory: "data"  # Subdirectory within user data dir for databases
    config_subdirectory: "config"  # Subdirectory within user config dir
    cache_subdirectory: "cache"  # Subdirectory within user cache dir
    logs_subdirectory: "logs"  # Subdirectory within user logs dir
    frontend_subdirectory: "frontend"  # Frontend state persistence and cache
  log_level: "INFO"

# Unified logging configuration - single source of truth for all subsystems
logging:
  # Log levels and filtering
  levels:
    default: "INFO"  # Default log level for all subsystems
    subsystems:
      cli: "INFO"
      backend: "INFO" 
      frontend: "INFO"
      studio: "INFO"
      modelservice: "INFO"
    modules:
      # Fine-grained control per module if needed
      security: "INFO"  # Security logs are important
      database: "INFO"
    externals:
      ollama: "INFO"    # Ollama server and model logs
      
  # Retention and cleanup
  retention:
    days: 7  # Keep logs for 7 days (reduced from 30 to manage volume)
    max_size_mb: 500  # Maximum total log storage size

# Message Bus Configuration
message_bus:
  host: "localhost"
  bind_address: "tcp://*:5555"
  pub_port: 5555  # Frontend port for publishers (broker binds here)
  sub_port: 5556  # Backend port for subscribers (broker binds here)
  timeout: 120.0  # Timeout for message bus requests (increased from 60s default)

# API Gateway configuration
api_gateway:
  enabled: true
  host: "127.0.0.1"
  port: 8771  # Used by modelservice for backend health checks
  
  # Main REST API configuration
  rest:
    port: 8771
    host: "127.0.0.1"

  # Protocol adapters
  protocols:
    websocket:
      enabled: true
      port: 8772
      path: "/ws"
    
    zeromq_ipc:
      enabled: true
  
  # Authentication
  auth:
    jwt:
      algorithm: "HS256"
      expiry_hours: 24
    session:
      cookie_name: "aico_session"
      timeout_minutes: 1440  # 24 hours
  
  # Rate limiting
  rate_limiting:
    default_requests_per_minute: 100
  
  # Plugin configuration
  plugins:
    message_bus:
      enabled: true
    log_consumer:
      enabled: true
    validation:
      enabled: true
    security:
      enabled: true
    rate_limiting:
      enabled: true
    encryption:
      enabled: true

# Modelservice configuration
modelservice:
  enabled: true
  rest:
    host: "127.0.0.1"
    port: 8773
  cors_origins:
    - "http://localhost:3000"
    - "http://127.0.0.1:3000"
  # Ollama server configuration
  ollama:
    # Connection settings - used by OllamaManager.is_running()
    host: "127.0.0.1"
    port: 11434
    
    # Auto-management settings - used by OllamaManager
    auto_install: true  # Automatically install Ollama binary if missing
    auto_start: true    # Start Ollama server with modelservice
    
    # Default models by capability role
    default_models:
      conversation:
        name: "huihui_ai/qwen3-abliterated:8b-v2"
        description: "Qwen3 Abliterated - Uncensored foundation model for character consistency"
        auto_pull: true
        auto_start: true  # Preload on startup for fast first response
        priority: 1
      # Note: Embeddings are handled by TransformersManager (sentence-transformers)
      # See modelservice.transformers.models.embeddings configuration below
      vision:
        name: "llama3.2-vision:11b"
        description: "Scene understanding and emotional context"
        auto_pull: false
        priority: 2
      lightweight:
        name: "llama3.2:1b"
        description: "Ultra-fast model for simple tasks"
        auto_pull: false
        priority: 4
    
    # Default model parameters
    default_parameters:
      temperature: 0.7
      max_tokens: 2048
      top_p: 0.9
      top_k: 40
      repeat_penalty: 1.1
      stop: []
    
    # Resource management
    resources:
      auto_unload_minutes: 30
      max_concurrent_models: 2
      memory_threshold_percent: 85
    
    # Ollama 0.12+ parallel processing configuration
    # These settings enable true parallel request batching
    num_parallel: 4  # Process up to 4 requests simultaneously per model (batched)
    max_loaded_models: 2  # Keep 2 models in memory (conversation + embeddings)
    max_queue: 128  # Queue depth before rejecting requests
  
  # Entity extraction is now handled by GLiNER via the transformers system
  
  # Hugging Face Transformers configuration
  transformers:
    # Memory management settings
    max_memory_mb: 2048  # Maximum memory for all transformers models
    max_concurrent_models: 3  # Maximum number of models loaded simultaneously
    auto_unload: true  # Automatically unload models when memory is low
    
    # Model configurations (can override defaults in TransformersManager)
    models:
      # Only override default configurations when needed
      # The default configurations are defined in modelservice/core/transformers_manager.py
      
      # Override model_id for entity extraction if needed
      entity_extraction:
        model_id: "urchade/gliner_medium-v2.1"
        
      # Override model_id for intent classification if needed
      intent_classification:
        model_id: "xlm-roberta-base"
        
      # Override model_id for embedding model if needed
      # Note: Use 'embeddings' as the key name (matches DEFAULT_MODELS)
      embeddings:
        model_id: "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
  
  # Text-to-Speech (TTS) configuration
  tts:
    enabled: true
    
    # TTS Engine Selection
    # Options:
    #   - "xtts": Coqui XTTS v2 - High-quality voice cloning, slower (~20s for 700 chars)
    #   - "piper": Piper TTS - Ultra-fast synthesis (<300ms), no voice cloning
    engine: "piper"  # Testing Piper for speed
    
    # XTTS Configuration (used when engine="xtts")
    xtts:
      model: "xtts_v2"  # Coqui XTTS v2 - multilingual neural TTS
      
      # Voice configuration per language
      voices:
        en: "Daisy Studious"  # English female voice
        de: "Daisy Studious"  # Lighter, younger-sounding voice
        # Other supported languages: es, fr, it, pt, pl, tr, ru, nl, cs, ar, zh-cn, hu, ko, ja, hi
      
      # Custom voice cloning (optional)
      # Place WAV files in modelservice/assets/voices/
      custom_voice_path: null  # Path to custom voice WAV file (overrides built-in voices)
    
    # Piper Configuration (used when engine="piper")
    piper:
      # Voice models per language (downloaded automatically on first use)
      voices:
        en: "en_US-amy-medium"  # Amy voice - clear, professional English
        de: "de_DE-kerstin-low"  # Kerstin - German female voice (testing artifacts)
      
      # Voice quality settings
      # Options: x_low (fastest), low, medium (balanced), high (best quality)
      quality: "medium"  # Default quality level
    
    # Common synthesis parameters (applies to both engines)
    speed: 1.0  # Speech speed multiplier (0.5-2.0)
    
    # Language detection
    auto_detect_language: true  # Automatically detect language from text

# Task Scheduler configuration
scheduler:
  enabled: true
  max_concurrent_tasks: 10
  max_cpu_percent: 80
  max_memory_percent: 80
  scheduler_interval: 1.0  # seconds
  task_timeout: 3600  # 1 hour
  idle_threshold_cpu: 20      # percent
  idle_threshold_memory: 70   # percent

# User profile configuration
user_profiles:
  default_user_type: "person"

# Service Authentication Configuration
service_auth:
  # Default settings for all service tokens
  defaults:
    token_validity_hours: 8760  # 1 year
    issuer: "aico-system"
    require_component_identity: true
    validate_audience: true
    validate_expiry: true
  
  # Service permissions for API Gateway access
  services:
    modelservice:
      - "logs:write"
      - "health:read"
    frontend:
      - "logs:write"
      - "conversation:read"
      - "conversation:write"
    cli:
      - "admin:read"
      - "admin:write"
      - "logs:read"
      - "system:read"
    studio:
      - "logs:write"
      - "system:read"
      - "admin:read"

# Emotion Engine Configuration
emotion:
  # CPM appraisal sensitivity (0.0-1.0)
  appraisal_sensitivity: 0.7  # How sensitive to emotional triggers
  
  # Emotional regulation strength (0.0-1.0)
  regulation_strength: 0.3  # How much to regulate intense emotions (0.3 = 9% dampening)
  
  # Context-aware arousal amplification for existential threats
  # Based on LeDoux (1996) amygdala research and modern threat appraisal studies
  # When triggered, arousal is amplified AND inertia is reduced to 30% (amygdala override)
  # This allows acute threat responses to dominate emotional state
  threat_arousal_boost: 0.25  # Arousal multiplier for high-stakes threats (0.25 = 25% increase)
  
  # Emotional inertia (temporal dynamics) 
  # Based on Kuppens et al. (2010) and Scherer's CPM recursive appraisal
  inertia:
    enabled: true  # Enable emotional state persistence
    weight: 0.4  # Influence of previous state (0.0-1.0, healthy range: 0.3-0.5)
    reactivity: 0.6  # Influence of current appraisal (0.0-1.0, should sum to 1.0 with weight)
    decay_per_turn: 0.1  # Decay of previous state influence per conversation turn
    supportive_context_bias: true  # Maintain supportive tone after stress episodes
  
  # Feature flags
  enable_user_emotion_detection: false  # Phase 2+ feature
  enable_llm_conditioning: true  # Condition LLM responses with emotional state
  
  # State history
  max_history_size: 100  # Maximum emotional states to keep in history

# Conversation Engine Configuration
conversation:
  max_context_messages: 10
  response_timeout_seconds: 120.0  # Increased from 30s to match message bus timeout
  default_response_mode: "text_only"  # Valid values: text_only, multimodal
  
  # Feature flags for AI components
  features:
    enable_emotion_integration: true  # ENABLED - Phase 1 complete
    enable_personality_integration: false
    enable_memory_integration: true
    enable_embodiment: false
    enable_agency: false

# These features will be implemented in future versions
# For now, we keep the configuration lean and focused on what's actually used

# Core Services Configuration
services:
  # Message bus service configuration
  message_bus:
    enabled: true
    bind_address: "tcp://*:5555"
    timeout: 120.0
  
  # Task scheduler service configuration  
  task_scheduler:
    enabled: true
    max_concurrent_tasks: 10
    scheduler_interval: 1.0
    
  # Log consumer service configuration
  log_consumer:
    enabled: true
    buffer_size: 1000
    flush_interval: 5.0
    
  # Security service configuration
  security:
    enabled: true
    encryption: true
    
  # Emotion engine service configuration
  emotion_engine:
    enabled: true
    appraisal_sensitivity: 0.7
    regulation_strength: 0.8
  
  # Conversation engine service configuration
  conversation_engine:
    enabled: true
    memory_integration: true
    emotion_integration: true
    
  # Rate limiting service configuration
  rate_limiting:
    enabled: true
    default_requests_per_minute: 100
    
  # Validation service configuration
  validation:
    enabled: true
    strict_mode: false
    
  # Encryption service configuration
  encryption:
    enabled: true
    algorithm: "AES-256-GCM"

# Memory System Configuration
memory:
  # Working Memory (LMDB) - Session context and conversation history
  working:
    ttl_seconds: 2592000 # 30 days TTL for conversation continuity (was 24h/86400)
    # Named databases (sub-databases) within the LMDB environment
    named_databases:
      - "session_memory"
      - "user_sessions"
  
  # Semantic Memory (ChromaDB) - V3: Conversation segments with embeddings
  semantic:
    enabled: true  # Enable semantic memory
    embedding_timeout_seconds: 120.0  # Timeout for embedding generation
    # V3: Conversation segments collection
    collections:
      conversation_segments: "conversation_segments"  # Conversation history with embeddings
    # Note: Embedding model configured in modelservice.transformers.models.embeddings (line 190)
    max_results: 10  # Maximum number of segments to return from queries
    min_similarity: 0.4  # Minimum similarity threshold for hybrid score (0-1, higher = more strict)
    min_semantic_score: 0.35  # Minimum semantic score for relevance filtering (0-1)
                              # Documents below this are filtered as irrelevant (no results returned)
                              # Prevents false positives from keyword-only matches
    # Hybrid search fusion method
    fusion_method: "rrf"  # "rrf" (Reciprocal Rank Fusion) or "weighted" (score-based)
    rrf_rank_constant: 0  # RRF rank constant (0 = adaptive based on dataset size, or set 10-60 manually)
    # BM25 configuration
    bm25_min_idf: 0.6  # Minimum IDF threshold for query terms (0 = no filtering, higher = more aggressive)
                       # Filters common words (e.g., "today" with IDF≈0.55) to reduce false positives
    # Weighted fusion (legacy, used if fusion_method="weighted")
    semantic_weight: 0.7  # Weight for semantic similarity (0-1)
    bm25_weight: 0.3  # Weight for BM25 keyword matching (0-1)
    
    # Knowledge Graph configuration (Property Graph for structured memory)
    knowledge_graph:
      max_gleanings: 0  # Number of gleaning passes for completeness (0-2 recommended, 0 = single pass, 3x faster)
      llm_timeout_seconds: 120.0  # Timeout for LLM operations (4 parallel × 20s avg + buffer)
      
      # Entity resolution (deduplication) settings
      entity_resolution:
        similarity_threshold: 0.75  # Cosine similarity threshold for semantic blocking (0-1, higher = stricter)
        use_llm_matching: true  # Use LLM (Eve) for entity matching with chain-of-thought reasoning
        use_llm_merging: true  # Use LLM (Eve) for entity merging with conflict resolution (intelligent property merging)
  
  # Adaptive Memory System (AMS) - Brain-inspired memory consolidation and learning
  # Temporal Intelligence - Preference evolution and time-aware memory
  temporal:
    enabled: true
    metadata_retention_days: 365  # Keep temporal metadata for 1 year
    evolution_tracking: true  # Track preference evolution over time
    confidence_decay_rate: 0.001  # Confidence decay per day without access (0.1% per day)
    max_fact_variants: 3  # Maximum variants per fact (prevents bloat)
    variant_cleanup_days: 90  # Delete low-confidence variants after 90 days
  
  # Memory Consolidation - Brain-inspired transfer from working to semantic memory and KG extraction
  consolidation:
    enabled: true
    user_sharding_cycle_days: 7  # Process 1/7 of users each day (7-day cycle)
    max_concurrent_users: 4  # Maximum parallel consolidation jobs
    max_duration_minutes: 60  # Maximum duration per user
    replay_batch_size: 100  # Experiences per replay batch
    priority_alpha: 0.6  # Priority sampling weight (0.0 = uniform, 1.0 = greedy)
    
    # Idle detection for consolidation
    idle_detection:
      cpu_threshold_percent: 20  # Trigger when CPU < 20%
      idle_duration_seconds: 300  # 5 minutes continuous idle required
      check_interval_seconds: 60  # Check every minute
    
    # Knowledge Graph extraction settings
    kg_extraction:
      enabled: true
      batch_size: 50  # Maximum messages to process per run
      max_age_hours: 24  # Only process messages from last 24 hours
      max_concurrent_extractions: 4  # Parallel message processing (LLM queues internally)
  
  # Behavioral Learning - Skill-based interaction with RLHF (Phase 3)
  behavioral:
    enabled: true  # ENABLED - Phase 3 implementation complete
    learning_rate: 0.1  # How quickly confidence scores adjust to feedback
    exploration_rate: 0.1  # Probability of trying lower-confidence skills
    max_skills_per_user: 100  # Maximum learned skills per user
    skill_selection_timeout_ms: 10  # Max time for skill selection
    min_confidence_threshold: 0.3  # Don't use skills below this confidence
    preference_vector_dim: 16  # Number of explicit style dimensions
    num_context_buckets: 100  # Context bucketing for contextual learning
    
    # Feedback classification (multilingual, uses embeddings)
    feedback_classifier:
      similarity_threshold: 0.6  # Minimum cosine similarity for category match
      embedding_model: "paraphrase-multilingual-mpnet-base-v2"  # Reuse existing model
    
    # Contextual bandit learning (Thompson Sampling)
    contextual_bandit:
      enabled: true  # ENABLED - Phase 3 implementation complete
      algorithm: "thompson_sampling"  # Thompson Sampling for exploration/exploitation
      update_interval_hours: 24  # Run batch learning daily at 4 AM
      min_trajectories: 10  # Minimum feedback events required before updating skill confidence (prevents updates on insufficient data)
      prior_alpha: 1.0  # Beta distribution prior (successes)
      prior_beta: 1.0  # Beta distribution prior (failures)
    
    # Trajectory logging for learning
    trajectory_logging:
      enabled: true  # ENABLED - Phase 3 implementation complete
      retention_days: 90  # Archive after 90 days
      hard_delete_days: 365  # Delete after 1 year
      keep_feedback_indefinitely: true  # Never delete trajectories with feedback

# AI Processing Configuration
