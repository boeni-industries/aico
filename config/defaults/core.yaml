system:
  name: "AICO"
  environment: "development"
  # Path configuration - uses cross-platform path resolution
  paths:
    directory_mode: "auto"  # auto = OS-appropriate dirs, current = ./data, or explicit path
    data_subdirectory: "data"  # Subdirectory within user data dir for databases
    config_subdirectory: "config"  # Subdirectory within user config dir
    cache_subdirectory: "cache"  # Subdirectory within user cache dir
    logs_subdirectory: "logs"  # Subdirectory within user logs dir
    frontend_subdirectory: "frontend"  # Frontend state persistence and cache
  log_level: "INFO"

# Unified logging configuration - single source of truth for all subsystems
logging:
  # Log levels and filtering
  levels:
    default: "INFO"  # Default log level for all subsystems
    subsystems:
      cli: "INFO"
      backend: "INFO" 
      frontend: "INFO"
      studio: "INFO"
      modelservice: "INFO"
    modules:
      # Fine-grained control per module if needed
      security: "INFO"  # Security logs are important
      database: "INFO"
    externals:
      ollama: "INFO"    # Ollama server and model logs
      
  # Retention and cleanup
  retention:
    days: 7  # Keep logs for 7 days (reduced from 30 to manage volume)
    max_size_mb: 500  # Maximum total log storage size

# Message Bus Configuration
message_bus:
  host: "localhost"
  bind_address: "tcp://*:5555"
  pub_port: 5555  # Frontend port for publishers (broker binds here)
  sub_port: 5556  # Backend port for subscribers (broker binds here)
  timeout: 120.0  # Timeout for message bus requests (increased from 60s default)

# API Gateway configuration
api_gateway:
  enabled: true
  host: "127.0.0.1"
  port: 8771  # Used by modelservice for backend health checks
  
  # Main REST API configuration
  rest:
    port: 8771
    host: "127.0.0.1"

  # Protocol adapters
  protocols:
    websocket:
      enabled: true
      port: 8772
      path: "/ws"
    
    zeromq_ipc:
      enabled: true
  
  # Authentication
  auth:
    jwt:
      algorithm: "HS256"
      expiry_hours: 24
    session:
      cookie_name: "aico_session"
      timeout_minutes: 1440  # 24 hours
  
  # Rate limiting
  rate_limiting:
    default_requests_per_minute: 100
  
  # Plugin configuration
  plugins:
    message_bus:
      enabled: true
    log_consumer:
      enabled: true
    validation:
      enabled: true
    security:
      enabled: true
    rate_limiting:
      enabled: true
    encryption:
      enabled: true

# Modelservice configuration
modelservice:
  enabled: true
  rest:
    host: "127.0.0.1"
    port: 8773
  cors_origins:
    - "http://localhost:3000"
    - "http://127.0.0.1:3000"
  # Ollama server configuration
  ollama:
    # Connection settings - used by OllamaManager.is_running()
    host: "127.0.0.1"
    port: 11434
    
    # Auto-management settings - used by OllamaManager
    auto_install: true  # Automatically install Ollama binary if missing
    auto_start: true    # Start Ollama server with modelservice
    
    # Default models by capability role
    default_models:
      conversation:
        name: "huihui_ai/qwen3-abliterated:8b-v2"
        description: "Qwen3 Abliterated - Uncensored foundation model for character consistency"
        auto_pull: true
        auto_start: true  # Preload on startup for fast first response
        priority: 1
      # Note: Embeddings are handled by TransformersManager (sentence-transformers)
      # See modelservice.transformers.models.embeddings configuration below
      vision:
        name: "llama3.2-vision:11b"
        description: "Scene understanding and emotional context"
        auto_pull: false
        priority: 2
      lightweight:
        name: "llama3.2:1b"
        description: "Ultra-fast model for simple tasks"
        auto_pull: false
        priority: 4
    
    # Default model parameters
    default_parameters:
      temperature: 0.7
      max_tokens: 2048
      top_p: 0.9
      top_k: 40
      repeat_penalty: 1.1
      stop: []
    
    # Resource management
    resources:
      auto_unload_minutes: 30
      max_concurrent_models: 2
      memory_threshold_percent: 85
  
  # Entity extraction is now handled by GLiNER via the transformers system
  
  # Hugging Face Transformers configuration
  transformers:
    # Memory management settings
    max_memory_mb: 2048  # Maximum memory for all transformers models
    max_concurrent_models: 3  # Maximum number of models loaded simultaneously
    auto_unload: true  # Automatically unload models when memory is low
    
    # Model configurations (can override defaults in TransformersManager)
    models:
      # Only override default configurations when needed
      # The default configurations are defined in modelservice/core/transformers_manager.py
      
      # Override model_id for entity extraction if needed
      entity_extraction:
        model_id: "urchade/gliner_medium-v2.1"
        
      # Override model_id for intent classification if needed
      intent_classification:
        model_id: "xlm-roberta-base"
        
      # Override model_id for embedding model if needed
      # Note: Use 'embeddings' as the key name (matches DEFAULT_MODELS)
      embeddings:
        model_id: "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"

# Task Scheduler configuration
scheduler:
  enabled: true
  max_concurrent_tasks: 10
  max_cpu_percent: 80
  max_memory_percent: 80
  scheduler_interval: 1.0  # seconds
  task_timeout: 3600  # 1 hour
  idle_threshold_cpu: 20      # percent
  idle_threshold_memory: 70   # percent

# User profile configuration
user_profiles:
  default_user_type: "person"

# Service Authentication Configuration
service_auth:
  # Default settings for all service tokens
  defaults:
    token_validity_hours: 8760  # 1 year
    issuer: "aico-system"
    require_component_identity: true
    validate_audience: true
    validate_expiry: true
  
  # Service permissions for API Gateway access
  services:
    modelservice:
      - "logs:write"
      - "health:read"
    frontend:
      - "logs:write"
      - "conversation:read"
      - "conversation:write"
    cli:
      - "admin:read"
      - "admin:write"
      - "logs:read"
      - "system:read"
    studio:
      - "logs:write"
      - "system:read"
      - "admin:read"

# Conversation Engine Configuration
conversation:
  max_context_messages: 10
  response_timeout_seconds: 120.0  # Increased from 30s to match message bus timeout
  default_response_mode: "text_only"  # Valid values: text_only, multimodal
  
  # Feature flags for AI components
  features:
    enable_emotion_integration: false
    enable_personality_integration: false
    enable_memory_integration: true
    enable_embodiment: false
    enable_agency: false

# These features will be implemented in future versions
# For now, we keep the configuration lean and focused on what's actually used

# Core Services Configuration
services:
  # Message bus service configuration
  message_bus:
    enabled: true
    bind_address: "tcp://*:5555"
    timeout: 120.0
  
  # Task scheduler service configuration  
  task_scheduler:
    enabled: true
    max_concurrent_tasks: 10
    scheduler_interval: 1.0
    
  # Log consumer service configuration
  log_consumer:
    enabled: true
    buffer_size: 1000
    flush_interval: 5.0
    
  # Security service configuration
  security:
    enabled: true
    encryption: true
    
  # Conversation engine service configuration
  conversation_engine:
    enabled: true
    memory_integration: true
    
  # Rate limiting service configuration
  rate_limiting:
    enabled: true
    default_requests_per_minute: 100
    
  # Validation service configuration
  validation:
    enabled: true
    strict_mode: false
    
  # Encryption service configuration
  encryption:
    enabled: true
    algorithm: "AES-256-GCM"

# Memory System Configuration
memory:
  # Working Memory (LMDB) - Session context and conversation history
  working:
    ttl_seconds: 86400 # 24 hours TTL for conversation continuity
    # Named databases (sub-databases) within the LMDB environment
    named_databases:
      - "session_memory"
      - "user_sessions"
  
  # Semantic Memory (ChromaDB) - V3: Conversation segments with embeddings
  semantic:
    enabled: true  # Enable semantic memory
    embedding_timeout_seconds: 120.0  # Timeout for embedding generation
    # V3: Conversation segments collection
    collections:
      conversation_segments: "conversation_segments"  # Conversation history with embeddings
    # Note: Embedding model configured in modelservice.transformers.models.embeddings (line 190)
    max_results: 10  # Maximum number of segments to return from queries
    min_similarity: 0.4  # Minimum similarity threshold for hybrid score (0-1, higher = more strict)
    min_semantic_score: 0.35  # Minimum semantic score for relevance filtering (0-1)
                              # Documents below this are filtered as irrelevant (no results returned)
                              # Prevents false positives from keyword-only matches
    # Hybrid search fusion method
    fusion_method: "rrf"  # "rrf" (Reciprocal Rank Fusion) or "weighted" (score-based)
    rrf_rank_constant: 0  # RRF rank constant (0 = adaptive based on dataset size, or set 10-60 manually)
    # BM25 configuration
    bm25_min_idf: 0.6  # Minimum IDF threshold for query terms (0 = no filtering, higher = more aggressive)
                       # Filters common words (e.g., "today" with IDFâ‰ˆ0.55) to reduce false positives
    # Weighted fusion (legacy, used if fusion_method="weighted")
    semantic_weight: 0.7  # Weight for semantic similarity (0-1)
    bm25_weight: 0.3  # Weight for BM25 keyword matching (0-1)
    
    # Knowledge Graph configuration (Property Graph for structured memory)
    knowledge_graph:
      max_gleanings: 0  # Number of gleaning passes for completeness (0-2 recommended, 0 = single pass, 3x faster)
      llm_timeout_seconds: 60.0  # Timeout for LLM operations (relation extraction, entity matching)
      
      # Entity resolution (deduplication) settings
      entity_resolution:
        similarity_threshold: 0.85  # Cosine similarity threshold for semantic blocking (0-1, higher = stricter)
        use_llm_matching: true  # Use LLM (Eve) for entity matching with chain-of-thought reasoning
        use_llm_merging: false  # Use LLM (Eve) for entity merging with conflict resolution (disabled for 2x speedup)
  
  # Adaptive Memory System (AMS) - Brain-inspired memory consolidation and learning
  # Temporal Intelligence - Preference evolution and time-aware memory
  temporal:
    enabled: true
    metadata_retention_days: 365  # Keep temporal metadata for 1 year
    evolution_tracking: true  # Track preference evolution over time
    confidence_decay_rate: 0.001  # Confidence decay per day without access (0.1% per day)
    max_fact_variants: 3  # Maximum variants per fact (prevents bloat)
    variant_cleanup_days: 90  # Delete low-confidence variants after 90 days
  
  # Memory Consolidation - Brain-inspired transfer from working to semantic memory and KG extraction
  consolidation:
    enabled: true
    user_sharding_cycle_days: 7  # Process 1/7 of users each day (7-day cycle)
    max_concurrent_users: 4  # Maximum parallel consolidation jobs
    max_duration_minutes: 60  # Maximum duration per user
    replay_batch_size: 100  # Experiences per replay batch
    priority_alpha: 0.6  # Priority sampling weight (0.0 = uniform, 1.0 = greedy)
    
    # Idle detection for consolidation
    idle_detection:
      cpu_threshold_percent: 20  # Trigger when CPU < 20%
      idle_duration_seconds: 300  # 5 minutes continuous idle required
      check_interval_seconds: 60  # Check every minute
    
    # Knowledge Graph extraction settings
    kg_extraction:
      enabled: true
      batch_size: 50  # Maximum messages to process per run
      max_age_hours: 24  # Only process messages from last 24 hours
      max_concurrent_extractions: 4  # Parallel message processing (10x speedup)
  
  # Behavioral Learning - Skill-based interaction with RLHF (Phase 3 - not yet implemented)
  behavioral:
    enabled: false  # Disabled until Phase 3 implementation
    learning_rate: 0.1  # How quickly confidence scores adjust to feedback
    exploration_rate: 0.1  # Probability of trying lower-confidence skills
    max_skills_per_user: 100  # Maximum learned skills per user
    skill_selection_timeout_ms: 10  # Max time for skill selection
    min_confidence_threshold: 0.3  # Don't use skills below this confidence
    preference_vector_dim: 16  # Number of explicit style dimensions
    num_context_buckets: 100  # Context bucketing for contextual learning
    
    # Feedback classification (multilingual, uses embeddings)
    feedback_classifier:
      similarity_threshold: 0.6  # Minimum cosine similarity for category match
      embedding_model: "paraphrase-multilingual-mpnet-base-v2"  # Reuse existing model
    
    # Contextual bandit learning (Thompson Sampling)
    contextual_bandit:
      enabled: false  # Disabled until Phase 3
      algorithm: "thompson_sampling"  # Thompson Sampling for exploration/exploitation
      update_interval_hours: 24  # Run batch learning daily at 4 AM
      min_trajectories: 10  # Minimum trajectories needed for update
      prior_alpha: 1.0  # Beta distribution prior (successes)
      prior_beta: 1.0  # Beta distribution prior (failures)
    
    # Trajectory logging for learning
    trajectory_logging:
      enabled: false  # Disabled until Phase 3
      retention_days: 90  # Archive after 90 days
      hard_delete_days: 365  # Delete after 1 year
      keep_feedback_indefinitely: true  # Never delete trajectories with feedback

# AI Processing Configuration
