system:
  name: "AICO"
  environment: "development"
  # Path configuration - uses cross-platform path resolution
  paths:
    directory_mode: "auto"  # auto = OS-appropriate dirs, current = ./data, or explicit path
    data_subdirectory: "data"  # Subdirectory within user data dir for databases
    config_subdirectory: "config"  # Subdirectory within user config dir
    cache_subdirectory: "cache"  # Subdirectory within user cache dir
    logs_subdirectory: "logs"  # Subdirectory within user logs dir
    frontend_subdirectory: "frontend"  # Frontend state persistence and cache
  log_level: "INFO"

# Unified logging configuration - single source of truth for all subsystems
logging:
  # Log levels and filtering
  levels:
    default: "INFO"  # Default log level for all subsystems
    subsystems:
      cli: "INFO"
      backend: "INFO" 
      frontend: "INFO"
      studio: "INFO"
      modelservice: "INFO"
    modules:
      # Fine-grained control per module if needed
      security: "INFO"  # Security logs are important
      database: "INFO"
    externals:
      ollama: "INFO"    # Ollama server and model logs
      
  # Retention and cleanup
  retention:
    days: 30  # Keep logs for 30 days
    max_size_mb: 500  # Maximum total log storage size
    
  # Transport configuration
  transport:
    # All subsystems use ZeroMQ message bus for unified logging
    # Creates topic hierarchy: logs.api_gateway.auth, logs.cli.security, etc.
    # Enables selective subscription (logs.* = all, logs.api_gateway.* = specific)
    # Will expand with more topics like "metrics", "events", "alerts" over time
    zmq_topic: "logs"  # Base topic for all log messages
  
    
  # Bootstrap fallback configuration
  bootstrap:
    fallback_console: true  # Enable console output during bootstrap/failures
    fallback_temp_file: true  # Enable temporary file logging as fallback

# Message Bus Configuration
message_bus:
  host: "localhost"
  bind_address: "tcp://*:5555"
  pub_port: 5555  # Frontend port for publishers (broker binds here)
  sub_port: 5556  # Backend port for subscribers (broker binds here)
  timeout: 120.0  # Timeout for message bus requests (increased from 60s default)

# API Gateway configuration
api_gateway:
  enabled: true
  host: "127.0.0.1"
  port: 8771  # Used by modelservice for backend health checks
  
  # Main REST API configuration
  rest:
    port: 8771
    host: "127.0.0.1"

  # Protocol adapters
  protocols:
    websocket:
      enabled: true
      port: 8772
      path: "/ws"
    
    zeromq_ipc:
      enabled: true
  
  # Authentication
  auth:
    jwt:
      algorithm: "HS256"
      expiry_hours: 24
    session:
      cookie_name: "aico_session"
      timeout_minutes: 1440  # 24 hours
  
  # Rate limiting
  rate_limiting:
    default_requests_per_minute: 100
  
  # Plugin configuration
  plugins:
    message_bus:
      enabled: true
    log_consumer:
      enabled: true
    validation:
      enabled: true
    security:
      enabled: true
    rate_limiting:
      enabled: true
    encryption:
      enabled: true

# Modelservice configuration
modelservice:
  enabled: true
  rest:
    host: "127.0.0.1"
    port: 8773
  cors_origins:
    - "http://localhost:3000"
    - "http://127.0.0.1:3000"
  # Ollama server configuration
  ollama:
    # Connection settings - used by OllamaManager.is_running()
    host: "127.0.0.1"
    port: 11434
    
    # Auto-management settings - used by OllamaManager
    auto_install: true  # Automatically install Ollama binary if missing
    auto_start: true    # Start Ollama server with modelservice
    
    # Default models by capability role
    default_models:
      conversation:
        name: "hermes3:8b"
        description: "Nous Hermes 3 - Primary foundation model for character consistency"
        auto_pull: true
        priority: 1
      embedding:
        name: "paraphrase-multilingual"  # Referenced by CLI tools and semantic memory
        description: "Multilingual embedding model for semantic memory (via TransformersManager)"
        auto_pull: false  # Not using Ollama for embeddings, using transformers instead
        priority: 1
        dimensions: 768  # Used for ChromaDB collection creation
      vision:
        name: "llama3.2-vision:11b"
        description: "Scene understanding and emotional context"
        auto_pull: false
        priority: 2
      lightweight:
        name: "llama3.2:1b"
        description: "Ultra-fast model for simple tasks"
        auto_pull: false
        priority: 4
    
    # Default model parameters
    default_parameters:
      temperature: 0.7
      max_tokens: 2048
      top_p: 0.9
      top_k: 40
      repeat_penalty: 1.1
      stop: []
    
    # Resource management
    resources:
      auto_unload_minutes: 30
      max_concurrent_models: 2
      memory_threshold_percent: 85
  
  # Entity extraction is now handled by GLiNER via the transformers system
  
  # Hugging Face Transformers configuration
  transformers:
    # Memory management settings
    max_memory_mb: 2048  # Maximum memory for all transformers models
    max_concurrent_models: 3  # Maximum number of models loaded simultaneously
    auto_unload: true  # Automatically unload models when memory is low
    
    # Model configurations (can override defaults in TransformersManager)
    models:
      # Only override default configurations when needed
      # The default configurations are defined in modelservice/core/transformers_manager.py
      
      # Override model_id for entity extraction if needed
      entity_extraction:
        model_id: "urchade/gliner_medium-v2.1"
        
      # Override model_id for intent classification if needed
      intent_classification:
        model_id: "xlm-roberta-base"
        
      # Override model_id for embedding model if needed
      # Note: Use 'embeddings' as the key name (matches DEFAULT_MODELS)
      embeddings:
        model_id: "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"

# Task Scheduler configuration
scheduler:
  enabled: true
  max_concurrent_tasks: 10
  max_cpu_percent: 80
  max_memory_percent: 80
  scheduler_interval: 1.0  # seconds
  task_timeout: 3600  # 1 hour
  idle_threshold_cpu: 20      # percent
  idle_threshold_memory: 70   # percent

# User profile configuration
user_profiles:
  default_user_type: "person"

# Service Authentication Configuration
service_auth:
  # Default settings for all service tokens
  defaults:
    token_validity_hours: 8760  # 1 year
    issuer: "aico-system"
    require_component_identity: true
    validate_audience: true
    validate_expiry: true
  
  # Service permissions for API Gateway access
  services:
    modelservice:
      - "logs:write"
      - "health:read"
    frontend:
      - "logs:write"
      - "conversation:read"
      - "conversation:write"
    cli:
      - "admin:read"
      - "admin:write"
      - "logs:read"
      - "system:read"
    studio:
      - "logs:write"
      - "system:read"
      - "admin:read"

# Conversation Engine Configuration
conversation:
  max_context_messages: 10
  response_timeout_seconds: 120.0  # Increased from 30s to match message bus timeout
  default_response_mode: "text_only"  # Valid values: text_only, multimodal
  
  # Feature flags for AI components
  features:
    enable_emotion_integration: false
    enable_personality_integration: false
    enable_memory_integration: true
    enable_embodiment: false
    enable_agency: false

# These features will be implemented in future versions
# For now, we keep the configuration lean and focused on what's actually used

# Core Services Configuration
services:
  # Message bus service configuration
  message_bus:
    enabled: true
    bind_address: "tcp://*:5555"
    timeout: 120.0
  
  # Task scheduler service configuration  
  task_scheduler:
    enabled: true
    max_concurrent_tasks: 10
    scheduler_interval: 1.0
    
  # Log consumer service configuration
  log_consumer:
    enabled: true
    buffer_size: 1000
    flush_interval: 5.0
    
  # Security service configuration
  security:
    enabled: true
    encryption: true
    
  # Conversation engine service configuration
  conversation_engine:
    enabled: true
    memory_integration: true
    
  # Rate limiting service configuration
  rate_limiting:
    enabled: true
    default_requests_per_minute: 100
    
  # Validation service configuration
  validation:
    enabled: true
    strict_mode: false
    
  # Encryption service configuration
  encryption:
    enabled: true
    algorithm: "AES-256-GCM"

# Memory System Configuration
memory:
  # Working Memory (LMDB) - Session context and conversation history
  working:
    ttl_seconds: 86400 # 24 hours TTL for conversation continuity
    # Named databases (sub-databases) within the LMDB environment
    named_databases:
      - "session_memory"
      - "user_sessions"
  
  # Semantic Memory (ChromaDB) - V3: Conversation segments with embeddings
  semantic:
    enabled: true  # Enable semantic memory
    embedding_timeout_seconds: 120.0  # Timeout for embedding generation
    # V3: Conversation segments collection
    collections:
      conversation_segments: "conversation_segments"  # Conversation history with embeddings
    # Note: Embedding model configured in modelservice.transformers.models.embeddings (line 190)
    max_results: 10  # Maximum number of segments to return from queries
    min_similarity: 0.4  # Minimum similarity threshold for hybrid score (0-1, higher = more strict)
    min_semantic_score: 0.35  # Minimum semantic score for relevance filtering (0-1)
                              # Documents below this are filtered as irrelevant (no results returned)
                              # Prevents false positives from keyword-only matches
    # Hybrid search fusion method
    fusion_method: "rrf"  # "rrf" (Reciprocal Rank Fusion) or "weighted" (score-based)
    rrf_rank_constant: 0  # RRF rank constant (0 = adaptive based on dataset size, or set 10-60 manually)
    # BM25 configuration
    bm25_min_idf: 0.6  # Minimum IDF threshold for query terms (0 = no filtering, higher = more aggressive)
                       # Filters common words (e.g., "today" with IDF≈0.55) to reduce false positives
    # Weighted fusion (legacy, used if fusion_method="weighted")
    semantic_weight: 0.7  # Weight for semantic similarity (0-1)
    bm25_weight: 0.3  # Weight for BM25 keyword matching (0-1)

# AI Processing Configuration
