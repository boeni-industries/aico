# TTS Phase 4: Frontend Integration - Implementation Guide

## Status: Ready for Implementation

Phase 3 (Backend) is complete. Phase 4 requires Gateway API endpoint implementation before frontend can connect.

---

## What's Done

### Backend (âœ… Complete)
- Coqui XTTS integrated in modelservice
- TTS handler with streaming support
- ZMQ topic registered: `modelservice/tts/request/v1`
- Protobuf messages defined: `TtsRequest`, `TtsStreamChunk`

### Frontend (âœ… Prepared)
- Clean TTS repository stub
- TTS remote datasource created (placeholder)
- Audio player infrastructure ready (`just_audio`)
- State management ready

---

## What's Needed

### 1. Gateway API Endpoint

**File:** `/backend/api/v1/tts.py` (new file)

```python
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from aico.proto.aico_modelservice_pb2 import TtsRequest, TtsStreamChunk
from aico.core.topics import AICOTopics
import asyncio

router = APIRouter(prefix="/tts", tags=["tts"])

@router.post("/synthesize")
async def synthesize_tts(
    text: str,
    language: str = "en",
    speed: float = 1.0
):
    """
    Stream TTS audio synthesis.
    
    Returns audio chunks as they're generated by modelservice.
    """
    try:
        # Create TTS request
        request = TtsRequest(
            text=text,
            language=language,
            speed=speed
        )
        
        # Send to modelservice via ZMQ
        async def audio_stream():
            # Subscribe to TTS stream topic
            async for chunk in message_bus.request_stream(
                topic=AICOTopics.MODELSERVICE_TTS_REQUEST,
                request=request,
                response_topic=AICOTopics.MODELSERVICE_TTS_STREAM
            ):
                if chunk.is_final:
                    break
                yield chunk.audio_data
        
        return StreamingResponse(
            audio_stream(),
            media_type="audio/wav"
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**Register in:** `/backend/main.py`
```python
from api.v1 import tts
app.include_router(tts.router, prefix="/api/v1")
```

---

### 2. Frontend TTS Remote Datasource

**File:** `/frontend/lib/data/datasources/remote/tts_remote_datasource.dart`

Update the `synthesize()` method:

```dart
@override
Stream<Uint8List> synthesize({
  required String text,
  required String language,
  double speed = 1.0,
}) async* {
  try {
    AICOLog.info('ðŸŽ¤ Requesting TTS: $text');
    
    // Send request to Gateway
    final response = await _resilientApi.executeOperation<Stream<List<int>>>(
      () => _resilientApi.apiClient.requestStream(
        'POST',
        '/tts/synthesize',
        data: {
          'text': text,
          'language': language,
          'speed': speed,
        },
      ),
      operationName: 'TTS Synthesis',
    );
    
    if (response == null) {
      throw Exception('TTS request failed');
    }
    
    // Stream audio chunks
    await for (final chunk in response) {
      yield Uint8List.fromList(chunk);
    }
    
    AICOLog.info('âœ… TTS streaming complete');
    
  } catch (e, stackTrace) {
    AICOLog.error('TTS synthesis failed', error: e, stackTrace: stackTrace);
    rethrow;
  }
}
```

---

### 3. Frontend TTS Repository

**File:** `/frontend/lib/data/repositories/tts_repository_impl.dart`

Update the `speak()` method:

```dart
@override
Future<void> speak(String text) async {
  if (text.isEmpty || !_isAvailable) return;

  try {
    // Update state immediately (triggers avatar animation)
    _updateState(_currentState.copyWith(
      status: TtsStatus.speaking,
      currentText: text,
      progress: 0.0,
    ));
    
    // Request TTS from backend
    final audioStream = _remoteDataSource.synthesize(
      text: text,
      language: 'en',  // TODO: Get from user settings
      speed: 1.0,
    );
    
    // Collect audio chunks
    final audioChunks = <Uint8List>[];
    await for (final chunk in audioStream) {
      if (chunk.isNotEmpty) {
        audioChunks.add(chunk);
      }
    }
    
    // Concatenate all chunks
    final totalLength = audioChunks.fold<int>(0, (sum, chunk) => sum + chunk.length);
    final audioData = Uint8List(totalLength);
    var offset = 0;
    for (final chunk in audioChunks) {
      audioData.setRange(offset, offset + chunk.length, chunk);
      offset += chunk.length;
    }
    
    // Play audio
    await _audioPlayer!.setAudioSource(
      _WavAudioSource(audioData),
    );
    await _audioPlayer!.play();
    
    // Wait for playback to complete
    await _audioPlayer!.playerStateStream.firstWhere(
      (state) => state.processingState == ProcessingState.completed,
    );
    
    _updateState(_currentState.copyWith(
      status: TtsStatus.idle,
      currentText: null,
      progress: 1.0,
    ));
    
  } catch (e, stackTrace) {
    AICOLog.error('TTS speak failed', error: e, stackTrace: stackTrace);
    _updateState(_currentState.copyWith(
      status: TtsStatus.error,
      errorMessage: 'TTS failed: ${e.toString()}',
    ));
  }
}

// Add custom audio source for WAV data
class _WavAudioSource extends StreamAudioSource {
  final Uint8List _audioData;
  
  _WavAudioSource(this._audioData);
  
  @override
  Future<StreamAudioResponse> request([int? start, int? end]) async {
    start ??= 0;
    end ??= _audioData.length;
    
    return StreamAudioResponse(
      sourceLength: _audioData.length,
      contentLength: end - start,
      offset: start,
      stream: Stream.value(_audioData.sublist(start, end)),
      contentType: 'audio/wav',
    );
  }
}
```

---

### 4. Dependency Injection

**File:** `/frontend/lib/core/providers/tts_providers.dart` (if not exists, create)

```dart
import 'package:riverpod_annotation/riverpod_annotation.dart';
import 'package:aico_frontend/data/datasources/remote/tts_remote_datasource.dart';
import 'package:aico_frontend/data/repositories/tts_repository_impl.dart';
import 'package:aico_frontend/core/providers/networking_providers.dart';

part 'tts_providers.g.dart';

@riverpod
TtsRemoteDataSource ttsRemoteDataSource(TtsRemoteDataSourceRef ref) {
  final resilientApi = ref.watch(resilientApiServiceProvider);
  return TtsRemoteDataSourceImpl(resilientApi);
}

@riverpod
TtsRepositoryImpl ttsRepository(TtsRepositoryRef ref) {
  final remoteDataSource = ref.watch(ttsRemoteDataSourceProvider);
  return TtsRepositoryImpl(remoteDataSource);
}
```

Update `TtsRepositoryImpl` constructor to accept datasource:
```dart
class TtsRepositoryImpl implements TtsRepository {
  final TtsRemoteDataSource _remoteDataSource;
  
  TtsRepositoryImpl(this._remoteDataSource);
  // ... rest of implementation
}
```

---

## Testing Steps

### 1. Test Backend TTS Handler

```bash
# In modelservice directory
python -m pytest modelservice/tests/test_tts_handler.py -v
```

### 2. Test Gateway Endpoint

```bash
curl -X POST http://localhost:8771/api/v1/tts/synthesize \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello world", "language": "en"}' \
  --output test.wav
```

### 3. Test Frontend Integration

1. Start backend + modelservice
2. Run Flutter app
3. Press "Read Aloud" button
4. Verify:
   - Avatar animation starts immediately
   - Audio plays smoothly
   - No UI freezing

---

## Expected Behavior

1. **Button Press** â†’ Immediate avatar animation (no delay)
2. **Backend Request** â†’ TTS synthesis starts (~200-500ms)
3. **Audio Streaming** â†’ Chunks arrive and play
4. **Completion** â†’ Avatar returns to idle

**Total latency:** ~300-700ms (acceptable for TTS)

---

## Troubleshooting

### Audio doesn't play
- Check Gateway logs for TTS request
- Check modelservice logs for synthesis
- Verify audio format (WAV, 22050 Hz)

### Avatar animation doesn't trigger
- Check TTS state updates in logs
- Verify `TtsStatus.speaking` is set immediately

### UI still freezes
- Check if audio concatenation is blocking
- Consider streaming playback instead of buffering

---

## Next Steps

1. Implement Gateway `/api/v1/tts/synthesize` endpoint
2. Update frontend datasource with actual API call
3. Test end-to-end flow
4. Optimize chunk size for latency
5. Add error handling and retry logic

---

## Notes

- First TTS request will be slow (~5s) due to model loading
- Subsequent requests are fast (~200-500ms per chunk)
- Model auto-downloads on first run (~1.8GB)
- Voice cloning can be added later via `speaker_wav` parameter
